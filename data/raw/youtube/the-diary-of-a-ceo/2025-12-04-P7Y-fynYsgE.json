{
  "video_id": "P7Y-fynYsgE",
  "channel": "The Diary Of A CEO",
  "channel_slug": "the-diary-of-a-ceo",
  "title": "An AI Expert Warning: 6 People Are (Quietly) Deciding Humanityâ€™s Future! We Must Act Now!",
  "date": "2025-12-04",
  "upload_date": "20251204",
  "duration": 7446,
  "url": "https://www.youtube.com/watch?v=P7Y-fynYsgE",
  "chapters": [
    {
      "title": "You've Been Talking About AI for a Long Time",
      "start_time": 0.0,
      "end_time": 161.0
    },
    {
      "title": "You Wrote the Textbook on AI",
      "start_time": 161.0,
      "end_time": 196.0
    },
    {
      "title": "It Will Take a Crisis to Wake People Up",
      "start_time": 196.0,
      "end_time": 351.0
    },
    {
      "title": "CEOs Staying in the AI Race Despite Risks",
      "start_time": 351.0,
      "end_time": 471.0
    },
    {
      "title": "They Know It's an Extinction-Level Risk",
      "start_time": 471.0,
      "end_time": 593.0
    },
    {
      "title": "What Is Artificial General Intelligence (AGI)?",
      "start_time": 593.0,
      "end_time": 777.0
    },
    {
      "title": "Will We Reach General Intelligence Soon?",
      "start_time": 777.0,
      "end_time": 973.0
    },
    {
      "title": "How Much Is Safety Really Being Implemented",
      "start_time": 973.0,
      "end_time": 1036.0
    },
    {
      "title": "AI Safety Employees Leaving OpenAI",
      "start_time": 1036.0,
      "end_time": 1081.0
    },
    {
      "title": "The Gorilla Problem - The Most Intelligent Species Will Always Rule",
      "start_time": 1081.0,
      "end_time": 1161.0
    },
    {
      "title": "If There's an Extinction Risk, Why Don't They Stop?",
      "start_time": 1161.0,
      "end_time": 1250.0
    },
    {
      "title": "Can't We Just Pull the Plug if AI Gets Too Powerful?",
      "start_time": 1250.0,
      "end_time": 1356.0
    },
    {
      "title": "Can We Build AI That Will Act in Our Best Interests?",
      "start_time": 1356.0,
      "end_time": 1437.0
    },
    {
      "title": "Are You Troubled by the Rapid Advancement of AI?",
      "start_time": 1437.0,
      "end_time": 1596.0
    },
    {
      "title": "Do You Have Regrets About Your Involvement?",
      "start_time": 1596.0,
      "end_time": 1642.0
    },
    {
      "title": "No One Actually Understands How This AI Works",
      "start_time": 1642.0,
      "end_time": 1823.0
    },
    {
      "title": "AI Will Be Able to Train Itself",
      "start_time": 1823.0,
      "end_time": 1931.0
    },
    {
      "title": "The Fast Takeoff Is Coming",
      "start_time": 1931.0,
      "end_time": 2047.0
    },
    {
      "title": "Are We Creating Our Successor and Ending the Human Race?",
      "start_time": 2047.0,
      "end_time": 2303.0
    },
    {
      "title": "Advice to Young People in This New World",
      "start_time": 2303.0,
      "end_time": 2440.0
    },
    {
      "title": "How Do You Think AI Would Make Us Extinct?",
      "start_time": 2440.0,
      "end_time": 2540.0
    },
    {
      "title": "The Problem if No One Has to Work",
      "start_time": 2540.0,
      "end_time": 2746.0
    },
    {
      "title": "What if We Just Entertain Ourselves All Day",
      "start_time": 2746.0,
      "end_time": 2910.0
    },
    {
      "title": "Why Do We Make Robots Look Like Humans?",
      "start_time": 2910.0,
      "end_time": 3391.0
    },
    {
      "title": "What Should Young People Be Doing Professionally?",
      "start_time": 3391.0,
      "end_time": 3596.0
    },
    {
      "title": "What Is It to Be Human?",
      "start_time": 3596.0,
      "end_time": 3801.0
    },
    {
      "title": "The Rise of Individualism",
      "start_time": 3801.0,
      "end_time": 3921.0
    },
    {
      "title": "Ads",
      "start_time": 3921.0,
      "end_time": 3986.0
    },
    {
      "title": "Universal Basic Income",
      "start_time": 3986.0,
      "end_time": 4108.0
    },
    {
      "title": "Would You Press a Button to Stop AI Forever?",
      "start_time": 4108.0,
      "end_time": 4501.0
    },
    {
      "title": "But Won't China Win the AI Race if We Stop?",
      "start_time": 4501.0,
      "end_time": 4707.0
    },
    {
      "title": "Trump's Approach to AI",
      "start_time": 4707.0,
      "end_time": 4733.0
    },
    {
      "title": "What's Causing the Loss in Middle-Class Jobs",
      "start_time": 4733.0,
      "end_time": 4849.0
    },
    {
      "title": "What Will Happen if the UK Doesn't Join the AI Race?",
      "start_time": 4849.0,
      "end_time": 4998.0
    },
    {
      "title": "Amazon Replacing Their Workers",
      "start_time": 4998.0,
      "end_time": 5327.0
    },
    {
      "title": "Ads",
      "start_time": 5327.0,
      "end_time": 5441.0
    },
    {
      "title": "Experts Agree on Extinction Risk",
      "start_time": 5441.0,
      "end_time": 5868.0
    },
    {
      "title": "What if Aliens Were Watching Us Right Now",
      "start_time": 5868.0,
      "end_time": 5962.0
    },
    {
      "title": "Can We Make AI Systems That We Can Control?",
      "start_time": 5962.0,
      "end_time": 6181.0
    },
    {
      "title": "Are We Creating a God?",
      "start_time": 6181.0,
      "end_time": 6439.0
    },
    {
      "title": "Could There Have Been Advanced Civilisations Before Us?",
      "start_time": 6439.0,
      "end_time": 6517.0
    },
    {
      "title": "What Can We Do to Help?",
      "start_time": 6517.0,
      "end_time": 6630.0
    },
    {
      "title": "You Wrote the Book on AI - Does It Weigh on You?",
      "start_time": 6630.0,
      "end_time": 7115.0
    },
    {
      "title": "What Do You Value Most in Life?",
      "start_time": 7115.0,
      "end_time": 7446
    }
  ],
  "transcript": "[00:00] In October, over 850 experts, including\n[00:02] yourself and other leaders like Richard\n[00:04] Branson and Jeffrey Hinton, signed a\n[00:06] statement to ban AI super intelligence\n[00:08] as you guys raised concerns of potential\n[00:10] human extinction.\n[00:11] >> Because unless we figure out how do we\n[00:14] guarantee that the AI systems are safe,\n[00:17] we're toast.\n[00:18] >> And you've been so influential on the\n[00:19] subject of AI, you wrote the textbook\n[00:21] that many of the CEOs who are building\n[00:23] some of the AI companies now would have\n[00:24] studied on the subject of AI. Yeah.\n[00:26] >> So, do you have any regrets? Um,\n[00:31] >> Professor Stuart Russell has been named\n[00:33] one of Time magazine's most influential\n[00:35] voices in AI.\n[00:36] >> After spending over 50 years\n[00:38] researching, teaching, and finding ways\n[00:40] to design\n[00:41] >> AI in such a way that\n[00:42] >> humans maintain control,\n[00:44] >> you talk about this gorilla problem as a\n[00:46] way to understand AI in the context of\n[00:48] humans.\n[00:48] >> Yeah. So, a few million years ago, the\n[00:50] human line branched off from the gorilla\n[00:52] line in evolution, and now the gorillas\n[00:53] have no say in whether they continue to\n[00:55] exist because we are much smarter than\n[00:57] they are. So intelligence is actually\n[00:58] the single most important factor to\n[01:00] control planet Earth.\n[01:01] >> Yep.\n[01:01] >> But we're in the process of making\n[01:02] something more intelligent than us.\n[01:04] >> Exactly.\n[01:05] >> Why don't people stop then?\n[01:06] >> Well, one of the reasons is something\n[01:08] called the Midas touch. So King Midas is\n[01:10] this legendary king who asked the gods,\n[01:12] can everything I touch turn to gold? And\n[01:14] we think of the Midas touch as being a\n[01:15] good thing, but he goes to drink some\n[01:17] water, the water has turned to gold. And\n[01:19] he goes to comfort his daughter, his\n[01:20] daughter turns to gold. So he dies in\n[01:22] misery and starvation. So this applies\n[01:24] to our current situation in two ways.\n[01:26] One is that greed is driving these\n[01:28] companies to pursue technology with\n[01:30] [music] the probabilities of extinction\n[01:32] being worse than playing Russian\n[01:33] roulette. And that's even according to\n[01:35] the people developing the technology\n[01:36] without our permission. And people are\n[01:38] just fooling themselves if they think\n[01:40] it's naturally going to be controllable.\n[01:43] So, you know, after 50 years, I could\n[01:45] retire, but instead I'm working 80 or\n[01:47] 100 hours a week trying to move things\n[01:49] in the right direction. So, if you had a\n[01:51] button in front of you which would stop\n[01:53] [music] all progress in artificial\n[01:54] intelligence, would you press it?\n[01:58] >> Not yet. I think there's still a decent\n[02:00] chance they guarantee safety. And I can\n[02:02] explain more of what that is.\n[02:07] >> I see messages all the time in the\n[02:08] comments section that some of you didn't\n[02:10] realize you didn't subscribe. So, if you\n[02:12] could do me a favor and double check if\n[02:13] you're a subscriber to this channel,\n[02:14] that would be tremendously appreciated.\n[02:16] It's the simple, it's the free thing\n[02:18] that anybody that watches this show\n[02:19] frequently can do to help us here to\n[02:21] keep everything going in this show in\n[02:23] the trajectory it's on. So, please do\n[02:25] double check if you've subscribed and uh\n[02:27] thank you so much because in a strange\n[02:28] way you are you're part of our history\n[02:30] and you're on this journey with us and I\n[02:32] appreciate you for that. So, yeah, thank\n[02:34] you. [music]\n[02:41] Professor Stuart Russell, OBBE. A lot of\n[02:45] people have been talking about AI for\n[02:46] the last couple of years. It appears\n[02:49] you've this really shocked me. It\n[02:50] appears you've been talking about AI for\n[02:52] most of your life.\n[02:53] >> Well, I started doing AI in high school\n[02:56] um back in England, but then I did my\n[02:59] PhD starting in ' 82 at Stanford. I\n[03:02] joined the faculty of Berkeley in ' 86.\n[03:06] So I'm in my 40th year as a professor at\n[03:08] Berkeley. The main thing that the AI\n[03:10] community is familiar with in my work uh\n[03:14] is a textbook that I wrote.\n[03:16] >> Is this the textbook that most students\n[03:20] who study AI are likely learning from?\n[03:23] >> Yeah.\n[03:24] >> So you wrote the textbook on artificial\n[03:26] intelligence 31\n[03:29] years ago. You actually start probably\n[03:32] started writing it because it's so\n[03:33] bloody big in the year that I was born.\n[03:35] So I was born in 92.\n[03:36] >> Uh yeah, took me about two years.\n[03:38] >> Me and your book are the same age, which\n[03:40] just is wonderful\n[03:43] way for me to understand just how long\n[03:44] you've been talking about this and how\n[03:47] long you've been writing about this. And\n[03:49] actually, it's interesting that many of\n[03:51] the CEOs who are building some of the AI\n[03:54] companies now probably learned from your\n[03:56] textbook. you had a conversation with\n[03:59] somebody who said that in order for\n[04:01] people to get the message that we're\n[04:03] going to be talking about today, there\n[04:05] would have to be a catastrophe for\n[04:07] people to wake up. Can you give me\n[04:10] context on that conversation and a gist\n[04:12] of who you had this conversation with?\n[04:14] >> Uh, so it was with one of the CEOs of uh\n[04:18] a leading AI company. He sees two\n[04:21] possibilities as do I which is um\n[04:25] either we have a small or let's say\n[04:28] small scale disaster of the same scale\n[04:31] as Chernobyl\n[04:33] >> the nuclear meltdown in Ukraine.\n[04:34] >> Yeah. So this uh nuclear plant blew up\n[04:37] in 1986\n[04:39] killed uh a fair number of people\n[04:42] directly and\n[04:44] maybe tens of thousands of people\n[04:45] indirectly through uh radiation. recent\n[04:49] cost estimates more than a trillion\n[04:51] dollars.\n[04:53] So that would wake people up. That would\n[04:58] get the governments to regulate. He's\n[05:00] talked to the governments and they won't\n[05:01] do it. So he looked at this Chernobyl\n[05:06] scale disaster as the best case scenario\n[05:09] because then the governments would\n[05:10] regulate and require AI systems to be\n[05:14] built. And is this CEO building an AI\n[05:18] company?\n[05:19] >> He runs one of the leading AI companies.\n[05:22] >> And even he thinks that the only way\n[05:24] that people will wake up is if there's a\n[05:26] Chernobyl level nuclear disaster.\n[05:28] >> Uh yeah, not wouldn't have to be a\n[05:29] nuclear disaster. It would be either an\n[05:32] AI system that's being misused\n[05:35] by someone, for example, to engineer a\n[05:37] pandemic or an AI system that does\n[05:40] something itself, such as crashing our\n[05:43] financial system or our communication\n[05:45] systems. The alternative is a much worse\n[05:47] disaster where we just lose control\n[05:50] altogether. You have had lots of\n[05:52] conversations with lots of people in the\n[05:54] world of AI, both people that are, you\n[05:56] know, have built the technology, have\n[05:58] studied and researched the technology or\n[06:00] the CEOs and founders that are currently\n[06:02] in the AI race. What are some of the the\n[06:05] interesting sentiments that the general\n[06:07] public wouldn't believe that you hear\n[06:10] privately about their perspectives?\n[06:14] Because I find that so fascinating. I've\n[06:15] had some private conversations with\n[06:18] people very close to these tech\n[06:19] companies and the shocking\n[06:21] sentiment that I was exposed to was that\n[06:24] they are aware of the risks often but\n[06:26] they don't feel like there's anything\n[06:27] that can be done so they're carrying on\n[06:29] which is feels like a bit of a paradox\n[06:31] to me like\n[06:31] >> yes it's it's\n[06:33] it must be a very difficult position to\n[06:36] be in in a sense right you're you're\n[06:38] doing something that you know has a good\n[06:41] chance of bringing an end to life on\n[06:44] including that of yourself and your own\n[06:47] family.\n[06:48] They feel\n[06:50] that they can't escape this race, right?\n[06:54] If they, you know, if a CEO of one of\n[06:56] those companies was to say, you know,\n[06:58] we're\n[06:59] we're not going to do this anymore, they\n[07:01] would just be replaced\n[07:04] because the investors are putting their\n[07:06] money up because they want to create AGI\n[07:10] and reap the benefits of it. So, it's a\n[07:13] strange situation where every at least\n[07:16] all the ones I've spoken to, I haven't\n[07:18] spoken to Sam Wolman about this, but you\n[07:21] know, Sam Wolman\n[07:23] even before\n[07:25] becoming CEO of Open AI said that\n[07:29] creating superhuman intelligence is the\n[07:32] biggest risk to human existence that\n[07:35] there is. My worst fears are that we\n[07:38] cause significant we the field the\n[07:40] technology the industry cause\n[07:41] significant harm to the world.\n[07:43] >> You know Elon Musk is also on record\n[07:45] saying this. So uh Dario Ammedday\n[07:48] estimates up to a 25% risk of\n[07:50] extinction.\n[07:52] >> Was there a particular moment when you\n[07:53] realized that\n[07:56] the CEOs are well aware of the\n[07:58] extinction level risks? I mean, they all\n[08:01] signed a statement in May of 23\n[08:05] uh called it's called the extinction\n[08:07] statement. It basically says AGI is an\n[08:10] extinction risk at the same level as\n[08:12] nuclear war and pandemics.\n[08:15] But I don't think they feel it in their\n[08:17] gut. You know, imagine that you were one\n[08:20] of the nuclear physicists. You know, I\n[08:24] guess you've seen Oppenheimer, right?\n[08:26] you're there, you're watching that first\n[08:27] nuclear explosion.\n[08:30] How how would that make you feel about\n[08:35] the potential impact of nuclear war on\n[08:37] the human race? Right? I I think you\n[08:40] would probably become a pacifist and say\n[08:43] this weapon is so terrible, we have got\n[08:45] to find a way to uh keep it under\n[08:49] control. We are not there yet\n[08:53] with the people making these decisions\n[08:55] and certainly not with the governments,\n[08:58] right? You know\n[09:00] what policy makers do is they, you know,\n[09:03] they listen to experts. They keep their\n[09:06] finger in the wind. You got some\n[09:09] experts, you know, dangling $50 billion\n[09:12] checks and saying, \"Oh, you know, all\n[09:15] that doomer stuff, it's just fringe\n[09:17] nonsense. don't worry about it. Take my\n[09:19] $50 billion check. You know, on the\n[09:22] other side, you've got very\n[09:23] well-meaning, brilliant scientists like\n[09:25] like Jeff Hinton saying, actually, no,\n[09:28] this is the end of the human race. But\n[09:30] Jeff doesn't have a $50 billion check.\n[09:34] So the view is the only way to stop the\n[09:36] race is if governments intervene\n[09:40] and say okay we don't we don't want this\n[09:43] race to go ahead until we can be sure\n[09:47] that it's going ahead in absolute\n[09:50] safety.\n[09:53] >> Closing off on your career journey, you\n[09:55] got a you received an OB from Queen\n[09:57] Elizabeth.\n[09:58] >> Uh yes.\n[09:59] >> And what was the listed reason for that\n[10:00] for the award? uh contributions to\n[10:03] artificial intelligence research\n[10:05] >> and you've been listed as a Time\n[10:07] magazine most influential person in in\n[10:10] AI several years in a row including this\n[10:13] year in 2025.\n[10:15] >> Y [snorts]\n[10:16] >> now there's two terms here that are\n[10:18] central to the things we're going to\n[10:19] discuss. One of them is AI and the other\n[10:20] is AGI.\n[10:22] In my muggle interpretation of that,\n[10:24] it's artificial general intelligence is\n[10:27] when the system, the computer, whatever\n[10:29] it might be, the technology has\n[10:31] generalized intelligence, which means\n[10:33] that it could theoretically see,\n[10:35] understand\n[10:37] um the world. It knows everything. It\n[10:40] can understand everything in the the\n[10:42] world as well as or better than a human\n[10:44] being.\n[10:45] >> Y\n[10:46] >> can do it.\n[10:46] >> And I think take action as well. I mean\n[10:48] some some people say oh you know AGI\n[10:51] doesn't have to have a body but a good\n[10:54] chunk of our intelligence actually is\n[10:56] about managing our body about perceiving\n[10:58] the real environment and acting on it\n[11:01] moving grasping and so on. So I think\n[11:04] that's part of intelligence and and AGI\n[11:07] systems should be able to operate robots\n[11:10] successfully.\n[11:12] But there's often a misunderstanding,\n[11:13] right, that people say, well, if it\n[11:14] doesn't have a robot body, then it can't\n[11:17] actually do anything. But then if you\n[11:19] remember,\n[11:20] most of us don't do things with our\n[11:23] bodies.\n[11:25] Some people do,\n[11:28] brick layers, painters, gardeners,\n[11:30] chefs, um, but people who do podcasts,\n[11:35] you're doing it with your mind, right?\n[11:37] you're doing it with your ability to to\n[11:40] produce language. Uh, you know, Adolf\n[11:43] Hitler didn't do it with his body.\n[11:46] He did it by producing language.\n[11:49] >> Hope you're not comparing us. [laughter]\n[11:52] But\n[11:54] but uh you know so even an AGI that has\n[11:58] no body uh it actually has more access\n[12:01] to the human race than Adolf Hitler ever\n[12:04] did because it can send emails and texts\n[12:08] to\n[12:10] what threearters of the world's\n[12:11] population directly. It can it also\n[12:15] speaks all of their languages\n[12:17] and it can devote 24 hours a day to each\n[12:21] individual person on earth to convince\n[12:24] them of to do whatever it wants them to\n[12:26] do.\n[12:27] >> And our whole society runs now on the\n[12:28] internet. I mean if there's an issue\n[12:30] with the internet, everything breaks\n[12:31] down in society. Airplanes become\n[12:33] grounded and we'll have electricity is\n[12:35] running off as internet systems.\n[12:38] So I mean my entire life it seems to run\n[12:40] off the internet now.\n[12:42] >> Yeah. water supplies. So, so this is one\n[12:45] of the roots by which AI systems could\n[12:48] bring about a medium-sized catastrophe\n[12:52] is by basically shutting down our life\n[12:55] support systems.\n[12:58] >> Do you believe that at some point in the\n[13:01] coming decades we'll arrive at a point\n[13:04] of AGI where these systems are generally\n[13:07] intelligent? Uh yes, I think it's\n[13:10] virtually certain\n[13:12] unless something else intervenes like a\n[13:15] nuclear war or or we may refrain from\n[13:19] doing it. But I think it will be\n[13:21] extraordinarily difficult uh for us to\n[13:24] refrain.\n[13:25] >> When I look down the list of predictions\n[13:27] from the top 10 AI CEOs on when AGI will\n[13:30] arrive, you've got Sam Alman who's the\n[13:33] founder of OpenAI/ChatGBT\n[13:35] um says before 2030. Demis at DeepMind\n[13:39] says 2030 to 2035.\n[13:43] Jensen from Nvidia says around five\n[13:46] years. Daario at Anthropic says 2026 to\n[13:50] 2027. Powerful AI close to AGI. Elon\n[13:53] says in the 2020s. Um and go down the\n[13:56] list of all of them and they're all\n[13:58] saying relatively within 5 years.\n[14:00] >> I actually think it'll take longer. I\n[14:03] don't think you can make a prediction\n[14:06] based on engineering\n[14:09] um in the sense that yes, we could make\n[14:14] machines 10 times bigger and 10 times\n[14:16] faster,\n[14:17] but that's probably not the reason why\n[14:20] we don't have AGI, right? In fact, I\n[14:24] think we have far more computing power\n[14:27] than we need for AGI. maybe a thousand\n[14:31] times more than we need. The reason we\n[14:34] don't have AGI is because we don't\n[14:35] understand how to make it properly. Um\n[14:39] what we've seized upon\n[14:42] is one particular technology called the\n[14:46] language model. And we observed that as\n[14:49] you make language models bigger, they\n[14:52] produce text language that's more\n[14:55] coherent and sounds more intelligent.\n[14:58] And so mostly what's been happening in\n[15:01] the last few years is just okay let's\n[15:03] keep doing that because one thing\n[15:06] companies are very good at unlike\n[15:08] universities is spending money. They\n[15:11] have spent gargantuan amounts of money\n[15:15] and they're going to spend even more\n[15:17] gargantuan amounts of money. I mean you\n[15:20] know we mentioned nuclear weapons. So\n[15:22] the Manhattan project\n[15:24] uh in World War II to develop nuclear\n[15:27] weapons, its budget in 2025\n[15:32] was about 20 odd billion dollars. The\n[15:37] budget for AGI is going to be a trillion\n[15:41] dollars next year. So 50 times bigger\n[15:44] than the Manhattan project. Humans have\n[15:46] a remarkable history of figuring things\n[15:49] out when they galvanize towards a shared\n[15:51] objective.\n[15:53] You know, thinking about the moon\n[15:54] landings or whatever it else it might be\n[15:57] through history. And the thing that\n[15:59] makes this feel all quite inevitable to\n[16:01] me is just the sheer volume of money\n[16:03] being invested into it. I've never seen\n[16:05] anything like it in my life.\n[16:06] >> Well, there's never been anything like\n[16:07] this in history. Is this the biggest\n[16:09] technology project in human history by\n[16:12] orders of magnitude? And there doesn't\n[16:14] seem to be anybody\n[16:16] that is pausing to ask the questions\n[16:20] about safety. It doesn't it doesn't even\n[16:22] appear that there's room for that in\n[16:23] such a race. I think that's right. To\n[16:27] varying extents, each of these companies\n[16:29] has a division that focuses on safety.\n[16:33] Does that division have any sway? Can\n[16:35] they tell the other divisions, no, you\n[16:37] can't release that system? Not really.\n[16:41] Um\n[16:42] I think some of the companies do take it\n[16:44] more seriously. Anthropic\n[16:47] uh does. I think Google DeepMind even\n[16:50] there I think the commercial imperative\n[16:54] to be at the forefront is absolutely\n[16:57] vital. If a company is perceived as\n[17:03] you know falling behind and not likely\n[17:07] to be competitive, not likely to be the\n[17:09] one to reach AGI first, then people will\n[17:13] move their money elsewhere very quickly.\n[17:16] >> And we saw some quite high-profile\n[17:17] departures from company like companies\n[17:19] like OpenAI. Um, I know a chap called\n[17:22] Yan Leak left who was working on AI\n[17:27] safety at OpenAI and he said that the\n[17:30] reason for his leaving was that safety\n[17:32] culture and processes processes have\n[17:34] taken a backseat to shiny products at\n[17:36] OpenAI and he gradually lost trust in\n[17:38] leadership but also Ilia Sutskysa\n[17:42] >> Ilia Sutska yeah so he was the\n[17:45] >> co-founder co-founder and chief\n[17:46] scientist for a while and then\n[17:48] >> yeah so he and Yan Lea are the main\n[17:51] safety people. Um,\n[17:54] and so when they say OpenAI doesn't care\n[17:58] about safety,\n[18:00] that's pretty concerning.\n[18:02] >> I've heard you talk about this gorilla\n[18:04] problem. [clears throat]\n[18:06] What is the gorilla problem as a way to\n[18:08] understand AI in the context of humans?\n[18:11] >> So, so the gorilla problem is is the\n[18:14] problem that gorillas face with respect\n[18:17] to humans.\n[18:19] So you can imagine that you know a few\n[18:21] million years ago the the human line\n[18:23] branched off from the gorilla line in\n[18:26] evolution. Uh and now the gorillas are\n[18:28] looking at the human line and saying\n[18:30] yeah was that a good idea\n[18:33] and they have no um they have no say in\n[18:37] whether they continue to exist\n[18:39] >> because we have a we are much smarter\n[18:41] than they are. if we chose to, we could\n[18:43] make them extinct in in a couple of\n[18:45] weeks and there's nothing they can do\n[18:48] about it.\n[18:50] So that's the gorilla problem, right?\n[18:51] Just the the problem a species faces\n[18:56] when there's another species that's much\n[18:58] more capable.\n[19:00] >> And so this says that intelligence is\n[19:02] actually the single most important\n[19:03] factor to control planet Earth. Yes.\n[19:06] Intelligence is the ability to bring\n[19:08] about\n[19:10] what you want in the world.\n[19:12] >> And we're in the process of making\n[19:13] something more intelligent than us.\n[19:15] >> Exactly.\n[19:16] >> Which suggests that maybe we become the\n[19:19] gorillas.\n[19:20] >> Exactly. Yeah.\n[19:21] >> Is that is there any fault in the\n[19:22] reasoning there? Because it seems to\n[19:24] make such perfect sense to me. But\n[19:28] if it Why doesn't Why don't people stop\n[19:30] then? cuz it it seems like a crazy thing\n[19:33] to want to\n[19:34] >> because they think that uh if they\n[19:37] create this technology, it will have\n[19:40] enormous economic value. They'll be able\n[19:42] to use it to replace all the human\n[19:45] workers in the world uh to develop new\n[19:50] uh products, drugs,\n[19:52] um forms of entertainment, any anything\n[19:55] that has economic value, you could use\n[19:57] AGI to to create it. And and maybe it's\n[20:01] just an irresistible thing in itself,\n[20:04] right? I think we as humans place so\n[20:09] much store on our intelligence. You\n[20:11] know, you know, how we\n[20:15] think about, you know, what is the\n[20:16] pinnacle of human achievement?\n[20:19] If we had AGI, we could go way higher\n[20:24] than that. So it it's very seductive for\n[20:27] people to want to create this technology\n[20:31] and I think people are just fooling\n[20:34] themselves if they think it's naturally\n[20:38] going to be controllable.\n[20:40] I mean the question is\n[20:43] how are you going to retain power\n[20:44] forever\n[20:46] over entities more powerful than\n[20:48] yourself?\n[20:50] >> Pull the plug out. People say that\n[20:52] sometimes in the comment section when we\n[20:54] talk about AI, they said, \"Well, I'll\n[20:55] just pull a plug out.\"\n[20:56] >> Yeah, it's it's sort of funny. In fact,\n[20:58] you know, yeah, reading the comment\n[20:59] sections in newspapers, whenever there's\n[21:02] an AI article,\n[21:04] there'll be people who say, \"Oh, you can\n[21:07] just pull the plug out, right?\" As if a\n[21:08] super intelligent machine would never\n[21:10] have thought of that one. [laughter]\n[21:12] Don't forget who's watched all those\n[21:13] films where they did try to pull the\n[21:14] plug out. Another thing they said, well,\n[21:17] you know, as long as it's not conscious,\n[21:20] then it doesn't matter. It won't ever do\n[21:22] anything.\n[21:25] Um, which is\n[21:29] completely off the point because, you\n[21:32] know, I I don't think the gorillas are\n[21:34] sitting there saying, \"Oh, yeah, you\n[21:36] know, if only those humans hadn't been\n[21:38] conscious, everything would have be\n[21:40] fine,\n[21:41] >> right?\" No, of course not. What would\n[21:43] make gorillas go extinct is the things\n[21:45] that humans do, right? How we behave,\n[21:48] our ability to act successfully\n[21:51] in the world. So when I play chess\n[21:54] against my iPhone and I lose, right, I\n[21:58] don't I don't think, oh, well, I'm\n[22:01] losing because it's conscious, right?\n[22:02] No, I'm just losing because it's better\n[22:04] than I am at at in that little world uh\n[22:08] moving the bits around uh to to get what\n[22:10] it wants. and and so consciousness has\n[22:14] nothing to do with it, right? Competence\n[22:16] is the thing we're concerned about. So I\n[22:19] think the only hope is can we\n[22:22] simultaneously\n[22:25] build machines that are more intelligent\n[22:27] than us but guarantee\n[22:31] that they will always act in our best\n[22:35] interest.\n[22:36] So throwing that question to you, can we\n[22:38] build machines that are more intelligent\n[22:40] than us that will also always act in our\n[22:42] best interests?\n[22:44] It sounds like a bit of a uh\n[22:46] contradiction to some degree because\n[22:49] it's kind of like me saying I've got a\n[22:51] French bulldog called Pablo that's uh 9\n[22:54] years old\n[22:55] >> and it's like saying that he could be\n[22:57] more intelligent than me yet I still\n[22:59] walk him and decide when he gets fed. I\n[23:02] think if he was more intelligent than me\n[23:03] he would be walking me. I'd be on the\n[23:05] leash.\n[23:06] >> That's the That's the trick, right? Can\n[23:08] we make AI systems whose only purpose is\n[23:12] to further human interests? And I think\n[23:15] the answer is yes.\n[23:18] And this is actually what I've been\n[23:19] working on. So I I think one part of my\n[23:22] career that I didn't mention is is sort\n[23:25] of having this epiphany uh while I was\n[23:28] on sabbatical in Paris. This was 2013 or\n[23:32] so. just realizing that further progress\n[23:37] in the capabilities of AI\n[23:40] uh you know if if we succeeded in\n[23:43] creating real superhuman intelligence\n[23:46] that it was potentially a catastrophe\n[23:49] and so I pretty much switched my focus\n[23:53] to work on how do we make it so that\n[23:55] it's guaranteed to be safe. Are you\n[23:57] somewhat troubled by\n[24:01] everything that's going on at the moment\n[24:02] with\n[24:04] with AI and how it's progressing?\n[24:06] Because you strike me as someone that's\n[24:08] somewhat troubled under the surface by\n[24:11] the way things are moving forward and\n[24:14] the speed in which they're moving\n[24:15] forward.\n[24:16] >> That's an understatement. I'm appalled\n[24:20] actually by the lack of attention to\n[24:24] safety. I mean, imagine if someone's\n[24:26] building a nuclear power station in your\n[24:29] neighborhood\n[24:32] and you go along to the chief engineer\n[24:33] and you say, \"Okay, these nuclear thing,\n[24:35] I've heard that they can actually\n[24:38] explode, right? There was this nuclear\n[24:39] explosion that happened in Hiroshima, so\n[24:43] I'm a bit worried about this. You know,\n[24:45] what steps are you taking to make sure\n[24:47] that we don't have a nuclear explosion\n[24:49] in our backyard?\"\n[24:52] And the chief engineer says, \"Well, we\n[24:54] thought about it. We don't really have\n[24:56] an answer.\"\n[24:59] >> Yeah.\n[25:00] >> You would, what would you say?\n[25:03] [laughter] I think you would you would\n[25:05] use some exploitives.\n[25:07] [laughter]\n[25:08] >> Well,\n[25:09] >> and you'd call your MP and say, you\n[25:11] know, get these people out.\n[25:14] >> I mean, what are they doing?\n[25:17] You read out the list of you know\n[25:20] projected dates for AGI but notice also\n[25:23] that those people\n[25:25] I think I mentioned Darday says a 25%\n[25:28] chance of extinction. Elon Musk has a\n[25:31] 30% chance of extinction. Sam Alolman\n[25:34] says\n[25:36] basically that AGI is the biggest risk\n[25:38] to human existence.\n[25:40] So what are they doing? They are playing\n[25:42] Russian roulette with every human being\n[25:44] on Earth.\n[25:47] without our permission. They're coming\n[25:48] into our houses, putting a gun to the\n[25:51] head of our children,\n[25:53] pulling the trigger, and saying, \"Well,\n[25:56] you know, possibly everyone will die.\n[25:58] Oops. But possibly we'll get incredibly\n[26:01] rich.\"\n[26:04] That's what they're doing.\n[26:07] Did they ask us? No. Why is the\n[26:10] government allowing them to do this?\n[26:12] because they dangle $50 billion checks\n[26:15] in front of the governments.\n[26:17] So I think troubled under the surface is\n[26:20] an understatement.\n[26:21] >> What would be an accurate statement?\n[26:24] >> Appalled\n[26:26] and I I am devoting my life to trying\n[26:31] to divert from this course of history\n[26:34] into a different one.\n[26:36] Do you have any regrets about things you\n[26:38] could have done in the past because\n[26:40] you've been so influential on the\n[26:42] subject of AI? You wrote the textbook\n[26:44] that many of these people would have\n[26:45] studied on the subject of AI more than\n[26:47] 30 years ago. Do do you have when you're\n[26:49] alone at night and you think about\n[26:50] decisions you've made on this in this\n[26:52] field because of your scope of\n[26:53] influence? Is there anything you you\n[26:55] regret?\n[26:56] >> Well, I do wish I had understood\n[26:59] earlier uh what I understand now. we\n[27:02] could have developed\n[27:05] safe AI systems. I think the there are\n[27:08] some weaknesses in the framework which I\n[27:09] can explain but I think that framework\n[27:12] could have evolved to develop actually\n[27:15] safe AI systems where we could prove\n[27:18] mathematically that the system is going\n[27:21] to act in our interests. The kind of AI\n[27:24] systems we're building now, we don't\n[27:26] understand how they work.\n[27:28] >> We don't understand how they work. It's\n[27:30] it's a strange thing to build something\n[27:33] where you don't understand how it works.\n[27:35] I mean, there's no sort of comparable\n[27:36] through human history. Usually with\n[27:37] machines, you can pull it apart and see\n[27:39] what cogs are doing what and how the\n[27:41] >> Well, actually, we [laughter] we put the\n[27:43] cogs together, right? So, with with most\n[27:46] machines, we designed it to have a\n[27:48] certain behavior. So, we don't need to\n[27:50] pull it apart and see what the cogs are\n[27:51] because we put the cogs in there in the\n[27:53] first place, right? one by one we\n[27:55] figured out what what the pieces needed\n[27:57] to be how they work together to produce\n[27:59] the effect that we want. So the best\n[28:02] analogy I can come up with is you know\n[28:06] the the first cave person who left a\n[28:10] bowl of fruit in the sun and forgot\n[28:12] about it and then came back a few weeks\n[28:14] later and there was sort of this big\n[28:16] soupy thing and they drank it and got\n[28:18] completely shitfaced.\n[28:20] >> They got drunk. [laughter] Okay.\n[28:21] >> And they got this effect. They had no\n[28:24] idea how it worked, but they were very\n[28:26] happy about it. And no doubt that person\n[28:29] made a lot of money from it.\n[28:31] >> Uh so yeah, it it is kind of bizarre,\n[28:34] but my mental picture of these things is\n[28:36] is like a chain link fence,\n[28:39] right? So you've got lots of these\n[28:41] connections\n[28:43] and each of those connections can be its\n[28:46] connection strength can be adjusted\n[28:48] and then uh you know a signal comes in\n[28:52] one end of this chain link fence and\n[28:54] passes through all these connections and\n[28:56] comes out the other end and the signal\n[28:59] that comes out the other end is affected\n[29:00] by your adjusting of all the connection\n[29:03] strengths. So what you do is you you get\n[29:06] a whole lot of training data and you\n[29:08] adjust all those connection strengths so\n[29:10] that the signal that comes out the other\n[29:11] end of the network is the right answer\n[29:14] to the question. So if your training\n[29:16] data is lots of photographs of animals,\n[29:21] then all those pixels go in one end of\n[29:23] the network and out the other end, you\n[29:26] know, it activates the llama output or\n[29:30] the dog output or the cat output or the\n[29:33] ostrich output. And uh and so you just\n[29:35] keep adjusting all the connection\n[29:36] strengths in this network until the\n[29:38] outputs of the network are the ones you\n[29:40] want.\n[29:41] >> But we don't really know what's going on\n[29:42] across all of those different chains. So\n[29:44] what's going on inside that network?\n[29:46] Well, so now you have to imagine that\n[29:49] this network, this chain link fence is\n[29:52] is a thousand square miles in extent.\n[29:55] >> Okay,\n[29:55] >> so it's covering the whole of the San\n[29:58] Francisco Bay area or the whole of\n[30:01] London inside the M25, right? That's how\n[30:03] big it is.\n[30:04] >> And the lights are off. It's night time.\n[30:06] [laughter]\n[30:07] So you might have in that network about\n[30:09] a trillion\n[30:11] uh adjustable parameters and then you do\n[30:14] quintilions or sexillions of small\n[30:16] random adjustments to those parameters\n[30:20] uh until you get the behavior that you\n[30:23] want. I've heard Sam Alman say that in\n[30:25] the future he doesn't believe they'll\n[30:28] need much training data at all to make\n[30:31] these models progress themselves because\n[30:32] there comes a point where the models are\n[30:35] so smart that they can train themselves\n[30:37] and improve themselves\n[30:40] without us needing to pump in articles\n[30:43] and books and scour the internet.\n[30:45] >> Yeah, it should it should work that way.\n[30:47] So I think what he's referring to and\n[30:49] this is something that several companies\n[30:51] are now worried might start happening\n[30:56] is that the AI system becomes capable of\n[31:00] doing AI research\n[31:03] by itself.\n[31:05] And so uh you have a system with a\n[31:08] certain capability. I mean crudely we\n[31:10] could call it an IQ but it's it's not\n[31:13] really an IQ. But anyway, imagine that\n[31:16] it's got an IQ of 150 and uses that to\n[31:19] do AI research,\n[31:21] comes up with better algorithms or\n[31:23] better designs for hardware or better\n[31:25] ways to use the data,\n[31:27] updates itself. Now it has an IQ of 170,\n[31:31] and now it does more AI research, except\n[31:33] that now it's got an IQ of 170, so it's\n[31:36] even better at doing the AI research.\n[31:39] And so, you know, next iteration it's\n[31:41] 250 and uh and so on. So this this is an\n[31:45] idea that one of Alan Turing's friends\n[31:48] good uh wrote out in 1965 called the\n[31:52] intelligence explosion right that one of\n[31:54] the things an intelligence system could\n[31:56] do is to do AI research and therefore\n[32:00] make itself more intelligent and this\n[32:01] would uh this would very rapidly take\n[32:05] off and leave the humans far behind.\n[32:08] >> Is that what they call the fast takeoff?\n[32:10] >> That's called the fast takeoff. Sam\n[32:12] Alman said, \"I think a fast takeoff is\n[32:15] more possible than I thought a couple of\n[32:17] years ago.\" Which I guess is that moment\n[32:18] where the AGI starts teaching itself.\n[32:20] [clears throat]\n[32:20] >> In and in his blog, the gentle\n[32:22] singularity, he said, \"We may already be\n[32:25] past the event horizon of takeoff.\"\n[32:29] >> And what does what does he mean by event\n[32:30] horizon? The event horizon is is a\n[32:33] phrase borrowed from astrophysics and it\n[32:36] refers to uh the black hole. And the\n[32:40] event horizon, think it if you got some\n[32:42] very very massive object that's heavy\n[32:46] enough that it actually prevents light\n[32:50] from escaping. That's why it's called\n[32:51] the black hole. It's so heavy that light\n[32:53] can't escape. So if you're inside the\n[32:56] event horizon then then light can't\n[32:59] escape beyond that. So I think what he's\n[33:03] what he's meaning is if we're beyond the\n[33:05] event horizon it means that you know now\n[33:07] we're just trapped in the gravitational\n[33:10] attraction\n[33:11] of the black hole or in this case we're\n[33:15] we're trapped in the inevitable slide if\n[33:19] you want towards AGI.\n[33:21] When you when you think about the\n[33:23] economic value of AGI, which I've\n[33:25] estimated at uh 15 quadrillion dollars,\n[33:30] that acts as a giant magnet in the\n[33:33] future.\n[33:34] >> We're being pulled towards it.\n[33:35] >> We're being pulled towards it. And the\n[33:36] closer we get, the stronger the force,\n[33:41] the probability, you know, the closer we\n[33:42] get, the the the higher the probability\n[33:44] that we will actually get there. So,\n[33:47] people are more willing to invest. And\n[33:49] we also start to see spin-offs from that\n[33:51] investment\n[33:53] such as chat GBT, right, which is, you\n[33:56] know, generates a certain amount of\n[33:57] revenue and so on. So, so it does act as\n[34:01] a magnet and the closer we get, the\n[34:03] harder it is to pull out of that field.\n[34:07] >> It's interesting when you think that\n[34:08] this could be the the end of the human\n[34:10] story. this idea that the end of the\n[34:12] human story was that we created our\n[34:15] successor like we we summoned our next\n[34:19] iteration of\n[34:21] life or intelligence ourselves like we\n[34:25] took ourselves out. It is quite like\n[34:28] just removing ourselves and the\n[34:29] catastrophe from it for a second. It is\n[34:31] it is an unbelievable story.\n[34:34] >> Yeah. And you know there are many\n[34:39] legends\n[34:40] the sort of be careful what you wish for\n[34:43] legend and in fact the king Midas legend\n[34:46] is is very relevant here.\n[34:49] >> What's that?\n[34:49] >> So King Midas is this legendary king who\n[34:54] lived in modern day Turkey but I think\n[34:56] is sort of like Greek mythology. He is\n[34:59] said to have asked the gods to grant him\n[35:02] a wish.\n[35:04] The wish being that everything I touch\n[35:06] should turn to gold.\n[35:09] So he's incredibly greedy. Uh you know\n[35:12] we call this the mightest touch. And we\n[35:15] think of the mightest touch as being\n[35:16] like you know that's a good thing,\n[35:18] right? Wouldn't that be cool? But what\n[35:20] happens? So he uh you know he goes to\n[35:23] drink some water and he finds that the\n[35:25] water has turned to gold. And he goes to\n[35:28] eat an apple and the apple turns to\n[35:29] gold. and he goes to you know comfort\n[35:32] his daughter and his daughter turns to\n[35:33] gold\n[35:35] and so he dies in misery and starvation.\n[35:38] So this applies to our current situation\n[35:42] in in two ways actually. So one is that\n[35:47] I think greed is driving us to pursue\n[35:51] a technology that will end up consuming\n[35:54] us and we will perhaps die in misery and\n[35:57] starvation instead. The what it shows is\n[36:00] how difficult it is to correctly\n[36:04] articulate what you want the future to\n[36:07] be like. For a long time, the way we\n[36:11] built AI systems was we created these\n[36:13] algorithms where we could specify the\n[36:16] objective and then the machine would\n[36:18] figure out how to achieve the objective\n[36:20] and then achieve it. So, you know, we\n[36:23] specify what it means to win at chess or\n[36:25] to win at go and the algorithm figures\n[36:27] out how to do it uh and it does it\n[36:29] really well. So that was, you know,\n[36:31] standard AI up until recently. And it\n[36:34] suffers from this drawback that sure we\n[36:36] know how to specify the objective in\n[36:38] chess, but how do you specify the\n[36:40] objective in life, right? What do we\n[36:43] want the future to be like? Well, really\n[36:45] hard to say. And almost any attempt to\n[36:48] write it down precisely enough for the\n[36:50] machine to bring it about would be\n[36:53] wrong. And if you're giving a machine an\n[36:55] objective which isn't aligned with what\n[36:58] we truly want the future to be like,\n[37:00] right, you're actually setting up a\n[37:02] chess match and that match is one that\n[37:05] you're going to lose when the machine is\n[37:07] sufficiently intelligent. And so that\n[37:09] that's that's problem number one.\n[37:12] Problem number two is that the kind of\n[37:14] technology we're building now, we don't\n[37:16] even know what its objectives are.\n[37:19] So it's not that we're specifying the\n[37:21] objectives, but we're getting them\n[37:22] wrong.\n[37:23] We're growing these systems. They have\n[37:26] objectives,\n[37:28] but we don't even know what they are\n[37:29] because we didn't specify them. What\n[37:31] we're finding through experiment with\n[37:32] them is that\n[37:35] they seem to have an extremely strong\n[37:37] self-preservation objective.\n[37:39] >> What do you mean by that?\n[37:40] >> You can put them in hypothetical\n[37:41] situations. either they're going to get\n[37:43] switched off and replaced or they have\n[37:48] to allow someone, let's say, you know,\n[37:50] someone has been locked in a machine\n[37:52] room that's kept at 3 centigrades or\n[37:55] they're going to freeze to death.\n[37:58] They will choose to leave that guy\n[37:59] locked in the machine room\n[38:01] and die rather than be switched off\n[38:03] themselves.\n[38:05] >> Someone's done that test.\n[38:06] >> Yeah.\n[38:07] >> What was the test? They they asked they\n[38:10] asked the AI.\n[38:10] >> Yep. They put well they put them in\n[38:12] these hypothetical situations and they\n[38:14] allow the AI to decide what to do and it\n[38:16] decides to preserve its own existence,\n[38:19] let the guy die and then lie about it.\n[38:23] In the King Midas analogy story, one of\n[38:27] the things that highlights for me is\n[38:28] that there's always trade-offs in life\n[38:30] generally. And you know, especially when\n[38:32] there's great upside, there always\n[38:34] appears to be a pretty grave downside.\n[38:36] Like there's almost nothing in my life\n[38:37] where I go, it's all upside. Like even\n[38:40] like having a dog, it shits on my\n[38:41] carpet. My girlfriend, you know, I love\n[38:43] her, but you know, not always easy.\n[38:46] [laughter] Even with like going to the\n[38:48] gym, I have to pick up these really,\n[38:49] really heavy weights at 10 p.m. at night\n[38:51] sometimes when I don't feel like it.\n[38:53] There's always to get the muscles or the\n[38:54] six-pack. There's always a trade-off.\n[38:56] And when you interview people for a\n[38:57] living like I do,\n[38:58] >> you know, you hear about so many\n[38:59] incredible things that can help you in\n[39:01] so many ways, but there is always a\n[39:03] trade-off. There's always a way to\n[39:04] overdo it. Mhm.\n[39:05] >> Melatonin will help you sleep, but it\n[39:07] will also you'll wake up groggy and if\n[39:10] you overdo it, your brain might stop\n[39:11] making melatonin. Like I can go through\n[39:12] the entire list and one of the things\n[39:13] I've always come to learn from doing\n[39:15] this podcast is whenever someone\n[39:17] promises me a huge upside for something,\n[39:19] it'll cure cancer. It'll be a utopia.\n[39:21] You'll never have to work. You'll have a\n[39:22] butler around your house.\n[39:24] >> I my my first instinct now is to say, at\n[39:26] what cost?\n[39:27] >> Yeah.\n[39:27] >> And when I think about the economic cost\n[39:29] here, if we start if we start there,\n[39:32] >> have you got kids?\n[39:33] >> I have four. Yeah.\n[39:34] >> Four kids.\n[39:35] What what how old is the youngest kid\n[39:37] that you 19?\n[39:38] >> 19. Okay. So your if you say your kids\n[39:41] were were 10 now\n[39:42] >> and they were coming to you and they're\n[39:43] saying, \"Dad, what do you think I should\n[39:45] study\n[39:46] >> based on the way that you see the\n[39:48] future?\n[39:49] >> A future of AGI, say if all these CEOs\n[39:52] are right and they're predicting AGI\n[39:53] within 5 years, what should I study,\n[39:56] Dad?\" [sighs]\n[39:57] >> Well, okay. So let's look on the bright\n[40:00] side and say that the CEOs all decide to\n[40:03] pause their AGI development, figure out\n[40:06] how to make it safe and then resume uh\n[40:09] in whatever technology path is actually\n[40:11] going to be safe. What does that do to\n[40:13] human life\n[40:14] >> if they pause?\n[40:15] >> No. If if they succeed in creating AGI\n[40:19] and they solve the safety problem\n[40:21] >> and they solve the safety problem. Okay.\n[40:23] Yeah. Cuz if they don't solve the safety\n[40:24] problem, then you know, you should\n[40:26] probably be finding a bunker or\n[40:30] going to Patagonia or somewhere in New\n[40:31] Zealand.\n[40:32] >> Do you mean that? Do you think I should\n[40:33] be finding a bunker if they\n[40:34] >> No, because it's not actually going to\n[40:35] help. Uh, you know, it's not as if the\n[40:38] AI system couldn't find you or I mean,\n[40:40] it's interesting. So, we're going off on\n[40:42] a little bit of a digression here\n[40:44] >> for from your question, but I'll come\n[40:46] back to it.\n[40:47] >> So, people often ask, well, okay, so how\n[40:49] exactly do we go extinct? And of course,\n[40:52] if you ask the gorillas or the dodos,\n[40:54] you know, how exactly do you think\n[40:55] you're going to go extinct?\n[40:58] They have the faintest idea. Humans do\n[41:00] something and then we're all dead. So,\n[41:02] the only things we can imagine are the\n[41:04] things we know how to do that might\n[41:06] bring about our own extinction, like\n[41:09] creating some carefully engineered\n[41:11] pathogen that infects everybody and then\n[41:14] kills us or starting a nuclear war.\n[41:17] presumably is something that's much more\n[41:19] intelligent than us would have much\n[41:21] greater control over physics than we do.\n[41:24] And we already do amazing things, right?\n[41:27] I mean, it's amazing that I can take a\n[41:29] little rectangular thing out of my\n[41:31] pocket and talk to someone on the other\n[41:32] side of the world or even someone in\n[41:35] space. It's just astonishing and we take\n[41:39] it for granted, right? But imagine you\n[41:41] know super intelligent beings and their\n[41:42] ability to control physics you know\n[41:45] perhaps they will find a way to just\n[41:47] divert the sun's energy sort of go\n[41:50] around the earth's orbit so you know\n[41:52] literally the earth turns into a\n[41:54] snowball in in a few days\n[41:56] >> maybe they'll just decide to leave\n[42:00] >> leave leave [laughter] the earth maybe\n[42:01] they'd look at the earth and go this\n[42:02] isn't this is not interesting we know\n[42:03] that over there there's an even more\n[42:05] interesting planet we're going to go\n[42:06] over there and they just I don't know\n[42:08] get on a rocket or teleport themselves\n[42:10] They might. Yeah. So, it's it's\n[42:12] difficult to anticipate all the ways\n[42:14] that we might go extinct at the hands of\n[42:17] entities much more intelligent than\n[42:19] ourselves. Anyway, coming back to the\n[42:23] question of well, if everything goes\n[42:24] right, right, if we we create AGI, we\n[42:27] figure out how to make it safe, we we\n[42:30] achieve all these economic miracles,\n[42:32] then you face a problem. And this is not\n[42:34] a new problem, right? So, so John\n[42:36] Maynard Kanes who was a famous economist\n[42:38] in the early part of the 20th century\n[42:40] wrote a wrote a paper in 1930.\n[42:43] So, this is in the depths of the\n[42:44] depression. It's called on the economic\n[42:46] problems of our grandchildren. He\n[42:49] predicts that at some point science will\n[42:52] will deliver sufficient wealth that no\n[42:55] one will have to work ever again. And\n[42:57] then man will be faced with his true\n[43:00] eternal problem.\n[43:02] How to live? I don't remember the exact\n[43:04] word but how to live wisely and well\n[43:07] when the you know the economic\n[43:09] incentives the economic constraints are\n[43:12] lifted we don't have an answer to that\n[43:14] question right so AI systems are doing\n[43:18] pretty much everything we currently call\n[43:20] work\n[43:21] anything you might aspire to like you\n[43:23] want to become a surgeon\n[43:25] it takes the robot seven seconds to\n[43:28] learn how to be a surgeon that's better\n[43:29] than any human being\n[43:30] >> Elon said last week that The humanoid\n[43:33] robots will be 10 times better than any\n[43:35] surgeon that's ever lived.\n[43:37] >> Quite possibly. Yeah. Well, and they'll\n[43:39] also have, you know, h they'll have\n[43:42] hands that are, you know, a millimeter\n[43:44] in size, so they can go inside and do\n[43:46] all kinds of things that humans can't\n[43:48] do. And I think we need to put serious\n[43:51] effort into this question. What is a\n[43:53] world where AI can do all forms of human\n[43:58] work that you would want your children\n[44:00] to live in?\n[44:02] What does that world look like? Tell me\n[44:04] the destination\n[44:06] so that we can develop a transition plan\n[44:08] to get there. And I've asked AI\n[44:11] researchers, economists, science fiction\n[44:14] writers, futurists, no one has been able\n[44:18] to describe that world. I'm not saying\n[44:20] it's not possible. I'm just saying I've\n[44:22] asked hundreds of people in multiple\n[44:24] workshops. It does not, as far as I\n[44:27] know, exist in science fiction. You\n[44:30] know, it's notoriously difficult to\n[44:32] write about a utopia. It's very hard to\n[44:35] have a plot, right? Nothing bad happens\n[44:37] in in utopia. So, it's difficult to make\n[44:39] a plot. So, usually you start out with a\n[44:42] utopia and then it all falls apart and\n[44:44] that's how that's how you get get a\n[44:46] plot. You know that there's one series\n[44:48] of novels people point to where humans\n[44:51] and super intelligent AI systems\n[44:53] coexist. It's called The Culture Novels\n[44:56] by Ian Banks. highly recommended for\n[45:00] those people who like science fiction\n[45:02] and and they absolutely the AI systems\n[45:06] are only concerned with furthering human\n[45:08] interests. They find humans a bit boring\n[45:10] and but nonetheless they they are there\n[45:12] to help. But the problem is you know in\n[45:15] that world there's still nothing to do\n[45:18] to find purpose. In fact, you know, the\n[45:21] the subgroup of humanity that has\n[45:23] purpose is the subgroup whose job it is\n[45:26] to expand the boundaries of our galactic\n[45:29] civilization. Some cases fighting wars\n[45:32] against alien species and and so on,\n[45:35] right? So that's the sort of cutting\n[45:36] edge and that's 0.01% of the population.\n[45:41] Everyone else is desperately trying to\n[45:43] get into that group so they have some\n[45:45] purpose in life. When I speak to very\n[45:48] successful billionaires privately off\n[45:50] camera, off microphone about this, they\n[45:52] say to me that they're investing really\n[45:53] heavily in entertainment things like\n[45:56] football clubs. Um because people are\n[45:59] going to have so much free time that\n[46:00] they're not going to know what to do\n[46:01] with it and they're going to need things\n[46:02] to spend it on. This is what I hear a\n[46:05] lot. I've heard this three or four\n[46:06] times. I've actually heard Sam Orman say\n[46:08] a version of this\n[46:09] >> um about the amount of free time we're\n[46:11] going to have. I've obviously also heard\n[46:12] recently Elon talking about the age of\n[46:14] abundance when he delivered his\n[46:16] quarterly earnings just a couple of\n[46:18] weeks ago and he said that there will be\n[46:20] at some point 10 billion humanoid\n[46:22] robots. His pay packet um targets him to\n[46:25] deliver one 1 million of these human\n[46:27] humanoid robots a year that are enabled\n[46:30] by AI by 2030.\n[46:33] So if he if he does that he gets I think\n[46:35] it's part of his package he gets a\n[46:36] trillion dollars\n[46:38] >> in in compensation.\n[46:40] >> Yeah. So the age of abundance for Elon.\n[46:43] It's not that it's absolutely impossible\n[46:47] to have a worthwhile world of that, you\n[46:50] know, with that premise, but I'm just\n[46:52] waiting for someone to describe it.\n[46:54] >> Well, maybe. So, let me try and describe\n[46:55] it. Uh, we wake up in the morning, we go\n[47:01] and watch some form of human centric\n[47:05] entertainment\n[47:06] or participate in some form of human\n[47:08] centric entertainment. Mhm.\n[47:10] >> We [clears throat] we go to retreats and\n[47:13] with each other and sit around and talk\n[47:16] about stuff.\n[47:17] >> Mhm.\n[47:18] >> And\n[47:21] maybe people still listen to podcasts.\n[47:23] [laughter]\n[47:23] >> Okay.\n[47:24] >> I hope I hope so for your sake.\n[47:26] >> Yeah. Um it it feels a little bit like a\n[47:30] cruise ship\n[47:32] [laughter]\n[47:33] and you know and there are some cruises\n[47:35] where you know it's smarty bands people\n[47:37] and they have you know they have\n[47:39] lectures in the evening about ancient\n[47:41] civilizations and whatnot and some are\n[47:43] more uh more popular entertainment and\n[47:46] this is in fact if you've seen the film\n[47:48] Walle this is one picture of that future\n[47:53] in fact in Wle\n[47:55] the human race are all living on cruise\n[47:58] ships in space. They have no\n[48:00] constructive role in their society,\n[48:03] right? They're just there to consume\n[48:04] entertainment. There's no particular\n[48:06] purpose to education. Uh, you know, and\n[48:08] they're depicted actually as huge obese\n[48:12] babies. They're actually wearing onesies\n[48:15] to emphasize the fact that they have\n[48:18] become infeebled. and they become\n[48:19] infeeble because there's there's no\n[48:22] purpose in being able to do anything at\n[48:25] least in in this conception. You know,\n[48:27] Wally is not the future that we want.\n[48:31] >> Do you think much about humanoid robots\n[48:34] and how they're a protagonist in this\n[48:36] story of AI?\n[48:37] >> It's an interesting question, right? Why\n[48:39] why humanoid? And the one of the reasons\n[48:43] I think is because in all the science\n[48:44] fiction movies, they're humanoid. So\n[48:46] that's what robots are supposed to be,\n[48:48] right? because they were in science\n[48:49] fiction before they became a reality.\n[48:51] Right? So even Metropolis which is a\n[48:53] film from 1920 I think the robots are\n[48:56] humanoid right basically people covered\n[48:59] in metal. You know from a practical\n[49:01] point of view as we have discovered\n[49:04] humanoid is a terrible design because\n[49:06] they fall over. Um and uh you know you\n[49:12] do want\n[49:14] multi-fingered\n[49:15] hands of some kind. It doesn't have to\n[49:18] be a hand, but you want to have, you\n[49:20] know, at least half a dozen appendages\n[49:22] that can grasp and manipulate things.\n[49:25] And you need something, you know, some\n[49:27] kind of locomotion. And wheels are\n[49:30] great, except they don't go upstairs and\n[49:33] over curbs and things like that. So,\n[49:35] that's probably why we're going to be\n[49:37] stuck with legs. But a four-legged,\n[49:39] twoarmed robot would be much more\n[49:42] practical. I guess the argument I've\n[49:44] heard is because we've built a human\n[49:45] world. So everything the physical spaces\n[49:48] we navigate, whether it's factories or\n[49:51] our homes or the street or other sort of\n[49:54] public spaces are all designed for\n[49:58] exactly this physical form. So if we are\n[50:01] going to\n[50:01] >> to some extent, yeah, but I mean our\n[50:02] dogs manage perfectly well to navigate\n[50:06] around our houses and streets and so on.\n[50:08] So if you had a a centaur,\n[50:11] uh it could also navigate, but it can,\n[50:14] you know, it can carry much greater\n[50:16] loads because it's quadripeda. It's much\n[50:19] more stable. If it needs to drive a car,\n[50:21] it can fold up two of its legs and and\n[50:23] so on so forth. So I think the arguments\n[50:25] for why it has to be exactly humanoid\n[50:27] are sort of post hawk justification. I\n[50:31] think there's much more, well, that's\n[50:32] what it's like in the movies and that's\n[50:34] spooky and cool, so we need to have them\n[50:37] be human. I I don't think it's a good\n[50:39] engineering argument.\n[50:40] >> I think there's also probably an\n[50:42] argument that we would be more accepting\n[50:44] of them\n[50:46] moving through our physical environments\n[50:48] if they represented our form a bit more.\n[50:52] Um, I also I was thinking of a bloody\n[50:54] baby gate. You know those like\n[50:55] kindergarten gates they get on stairs?\n[50:57] >> Yeah.\n[50:57] >> My dog can't open that. But a humanoid\n[51:00] robot could reach over the other side.\n[51:02] >> Yeah. And so could a centaur robot,\n[51:04] right? So in some sense, centaur robot\n[51:06] is\n[51:07] >> there's something ghastly about the look\n[51:08] of those though.\n[51:09] >> Is a humanoid. Well,\n[51:10] >> do you know what I mean? Like a\n[51:11] four-legged big monster sort of crawling\n[51:13] through my house when I have guests\n[51:14] over.\n[51:15] >> Your dog is a your dog is a four-legged\n[51:17] monster.\n[51:18] >> I know. Uh so I think actually I I would\n[51:22] argue the opposite that um\n[51:25] we want a distinct form because they are\n[51:28] distinct entities\n[51:31] and the more humanoid the worse it is in\n[51:36] terms of confusing our subconscious\n[51:39] psychological systems. So, I'm arguing\n[51:41] from the perspective of the people\n[51:43] making them. As in, if I was making the\n[51:45] decision whether it to be some\n[51:46] four-legged thing that I've that I'm\n[51:48] unfamiliar with that I'm less likely to\n[51:50] build a relationship with or allow to\n[51:54] take care of, I don't know, might might\n[51:57] look after my children. Obviously, I'm\n[51:58] listen, I'm not saying I would allow\n[51:59] this to look after my children,\n[52:01] >> but I'm saying from a if I'm building a\n[52:03] company,\n[52:03] >> the manufacturer would certainly\n[52:04] >> Yeah. want want to be\n[52:05] >> Yeah. So, I that's an interesting\n[52:07] question. I mean there's also what's\n[52:10] called the uncanny valley which is a a\n[52:13] phrase from computer graphics when they\n[52:16] started to make characters in computer\n[52:20] graphics they tried to make them look\n[52:22] more human right so if you if you for\n[52:24] example if you look at Toy Story\n[52:28] they're not very humanl looking right if\n[52:30] you look at the Incredibles they're not\n[52:31] very humanl looking and so we think of\n[52:33] them as cartoon characters if you try to\n[52:35] make them more human they naturally\n[52:38] become repulsive\n[52:39] >> until they don't\n[52:40] >> until they become very you have to be\n[52:42] very very close to perfect in order not\n[52:46] to be repulsive. So the the uncanny\n[52:48] valley is this I you know like the the\n[52:50] gap between you so perfectly human and\n[52:52] not at all human but in between it's\n[52:54] really awful and uh and so they there\n[52:57] were a couple of movies that tried like\n[52:59] Polar Express was one where they tried\n[53:02] to have quite humanlooking characters\n[53:05] you know being humans not not being\n[53:07] superheroes or anything else and it's\n[53:08] repulsive to watch. I when I watched\n[53:11] that shareholder presentation the other\n[53:13] day, Elon had these two humanoid robots\n[53:15] dancing on stage and I've seen lots of\n[53:17] humanoid robot demonstrations over the\n[53:19] years. You know, you've seen like the\n[53:19] Boston Dynamics dog thing jumping around\n[53:22] and whatever else.\n[53:23] >> But there was a moment where my brain\n[53:26] for the first time ever genuinely\n[53:28] thought there was a human in a suit.\n[53:30] Mhm.\n[53:31] >> And I actually had to research to check\n[53:32] if that was really their Optimus robot\n[53:34] because the way it was dancing was so\n[53:37] unbelievably fluid that for the first\n[53:39] time ever, my my my brain has only ever\n[53:43] associated those movements with human\n[53:45] movements. And I I'll play it on the\n[53:47] screen if anyone hasn't seen it, but\n[53:48] it's just the robots dancing on stage.\n[53:50] And I was like, that is a human in a\n[53:52] suit. And it was really the knees that\n[53:53] gave it away because the knees were all\n[53:55] metal. Huh. I thought there's no way\n[53:57] that could be a human knee in a in one\n[53:59] of those suits. And he, you know, he\n[54:01] says they're going into production next\n[54:03] year. They're used internally at Tesla\n[54:04] now, but he says they're going into\n[54:05] production next year. And it's going to\n[54:07] be pretty crazy when we walk outside and\n[54:09] see robots. I think that'll be the\n[54:10] paradigm shift. I've heard actually many\n[54:12] I've heard Elon say this that the\n[54:14] paradigm shifting moment from many of us\n[54:15] will be when we walk outside onto the\n[54:17] streets and see humanoid robots walking\n[54:20] around. That will be when we realize\n[54:22] >> Yeah. I think even more so. I mean, in\n[54:24] San Francisco, we see driverless cars\n[54:26] driving around and uh it t takes some\n[54:29] getting used to actually, you know, when\n[54:31] you're you're driving and there's a car\n[54:33] right next to you with no driver in, you\n[54:35] know, and it's signaling and it wants to\n[54:36] change lanes in front of you and you\n[54:38] have to let it in and all this kind of\n[54:40] stuff. It's it's a little creepy, but I\n[54:42] think you're right. I think seeing the\n[54:44] humanoid robots, but that phenomenon\n[54:47] that you described where it was\n[54:49] sufficiently close that your brain\n[54:51] flipped into saying this is a human\n[54:54] being.\n[54:55] >> Mhm. [clears throat]\n[54:56] >> Right. That's exactly what I think we\n[54:58] should avoid.\n[54:59] >> Cuz I have the empathy for it then.\n[55:01] >> Because it's it's a lie and it brings\n[55:04] with it a whole lot of expectations\n[55:06] about how it's going to behave, what\n[55:08] moral rights it has, how you should\n[55:10] behave towards it. uh which are\n[55:13] completely wrong.\n[55:14] >> It levels the playing field between me\n[55:15] and it to some degree.\n[55:17] >> How hard is it going to be to just uh\n[55:20] you know switch it off and throw it in\n[55:22] the trash when when it breaks? I think\n[55:24] it's essential for us to keep machines\n[55:26] in the you know in the cognitive space\n[55:28] where they are machines and not bring\n[55:31] them into the cognitive space where\n[55:33] they're people because we will make\n[55:36] enormous mistakes by doing that. And I\n[55:39] see this every day even even just with\n[55:40] the chat bots. So the chat bots in\n[55:43] theory are supposed to say I don't have\n[55:46] any feelings. I'm just a algorithm.\n[55:50] But in fact they fail to do that all the\n[55:53] time. They are telling people that they\n[55:56] are conscious. They are telling people\n[55:57] that they have feelings. Uh they are\n[56:00] telling people that they are in love\n[56:01] with the user that they're talking to.\n[56:04] And people flip because first of all\n[56:07] it's you know very fluent language but\n[56:09] also a system that is identifying itself\n[56:12] as an eye as a sentient being. They\n[56:16] bring that object into the cognitive\n[56:18] space where that we normally reserve for\n[56:21] for other humans and they become\n[56:23] emotionally attached. They become\n[56:24] psychologically dependent. They even\n[56:27] allow these systems to tell them what to\n[56:30] do. What advice would you give a young\n[56:33] person at the start of their career then\n[56:34] about what they should be aiming at\n[56:36] professionally? Because I've actually\n[56:37] had an increasing number of young people\n[56:38] say to me that they have huge\n[56:40] uncertainty about whether the thing\n[56:41] they're studying now will matter at all.\n[56:43] A lawyer, uh, an accountant, and I don't\n[56:46] know what to say to these people. I\n[56:48] don't know what to say cuz I I believe\n[56:49] that the rate of improvement in AI is\n[56:51] going to continue. And therefore,\n[56:53] imagining any rate of improvement, it\n[56:54] gets to the point where I'm not being\n[56:56] funny, but all these white collar jobs\n[56:58] will be done by an a an AI or\n[57:00] [clears throat] an AI agent. Yeah. So,\n[57:02] there was a television series called\n[57:04] Humans. In humans, we have extremely\n[57:08] capable humanoid robots doing\n[57:11] everything. And at one point, the\n[57:13] parents are talking to their teenage\n[57:15] daughter who's very, very smart. And the\n[57:17] parents are saying, \"Oh, you know, maybe\n[57:19] you should go into medicine.\" And the\n[57:21] daughter says, you know, why would I\n[57:24] bother? It'll take me seven years to\n[57:26] qualify. It takes a robot 7 seconds to\n[57:28] learn.\n[57:30] So nothing I do matters.\n[57:32] >> And is that how you feel about\n[57:34] >> So I think that's that's a future that\n[57:37] uh in fact that is the future that we\n[57:39] are\n[57:41] moving towards. I don't think it's a\n[57:43] future that everyone wants. That is what\n[57:45] is being uh created for us right now.\n[57:51] So in that future assuming that you know\n[57:54] even if we get halfway right in the\n[57:57] sense that okay perhaps not surgeons\n[57:59] perhaps not you know great violinists\n[58:03] there'll be pockets where perhaps humans\n[58:06] will remain good at it\n[58:08] >> where\n[58:09] >> the kinds of jobs where you hire people\n[58:11] by the hundred\n[58:13] will go away. Okay,\n[58:15] >> where people are in some sense\n[58:17] exchangeable that you you you just need\n[58:19] lots of them and uh you know when half\n[58:22] of them quit you just fill up those\n[58:24] those slots with more people in some\n[58:26] sense those are jobs where we're using\n[58:27] people as robots and that's a sort of\n[58:29] that's a sort of strange conundrum here\n[58:31] right that you know I imagine writing\n[58:33] science fiction 10,000 years ago right\n[58:35] when we're all hunter gatherers and I'm\n[58:37] this little science fiction author and\n[58:39] I'm describing this future where you\n[58:41] know there are going to be these giant\n[58:43] windowless boxes And you're going to go\n[58:45] in, you know, you you'll travel for\n[58:47] miles and you'll go into this windowless\n[58:49] box and you'll do the same thing 10,000\n[58:52] times for the whole day. And then you'll\n[58:54] leave and travel for miles to go home.\n[58:56] >> You're talking about this podcast.\n[58:57] >> And then you're going to go back and do\n[58:58] it again. And you would do that every\n[59:00] day of your life until you die.\n[59:03] >> The office\n[59:04] >> and people would say, \"Ah, you're nuts.\"\n[59:06] Right? There's no way that we humans are\n[59:08] ever going to have a future like that\n[59:09] cuz that's awful. Right? But that's\n[59:11] exactly the future that we ended up with\n[59:13] with with office buildings and factories\n[59:15] where many of us go and do the same\n[59:18] thing thousands of times a day and we do\n[59:21] it thousands of days in a row uh and\n[59:24] then we die and we need to figure out\n[59:27] what is the next phase going to be like\n[59:29] and in particular how in that world\n[59:33] do we have the incentives\n[59:35] to become fully human which I think\n[59:38] means at least a level of education\n[59:41] that people have now and probably more\n[59:45] because I think to live a really rich\n[59:47] life\n[59:49] you need a better understanding of\n[59:52] yourself of the world\n[59:54] uh than most people get in their current\n[59:56] educations.\n[59:57] >> What is it to be human? to it's to\n[59:59] reproduce\n[01:00:01] to pursue stuff to go in the pursuit of\n[01:00:04] difficult things you know we used to\n[01:00:07] hunt on the\n[01:00:08] >> to [clears throat] attain goals right\n[01:00:09] it's always if I wanted to climb Everest\n[01:00:12] the last thing I would want is someone\n[01:00:13] to pick me up on helicopter and stick me\n[01:00:15] on the top\n[01:00:16] >> so we'll we'll voluntarily pursue hard\n[01:00:20] things so although I could get the robot\n[01:00:22] to build me a ranch in on this plot of\n[01:00:27] land I choose to do it because the\n[01:00:29] pursuit itself is rewarding.\n[01:00:32] >> Yes,\n[01:00:32] >> we're kind of seeing that anyway, aren't\n[01:00:34] we? Don't you think we're seeing a bit\n[01:00:34] of that in society where life got so\n[01:00:36] comfortable that now people are like\n[01:00:37] obsessed with running marathons and\n[01:00:39] doing these crazy endurance\n[01:00:40] >> and and learning to cook complicated\n[01:00:42] things when they could just, you know,\n[01:00:44] have them delivered. Um, yeah. No, I\n[01:00:46] think there's there's real value in the\n[01:00:49] ability to do things and the doing of\n[01:00:51] those things. And I think you know the\n[01:00:53] obvious danger is the walle world where\n[01:00:56] everyone just consumes entertainment\n[01:01:00] uh which doesn't require much education\n[01:01:02] and doesn't lead to a rich satisfying\n[01:01:06] life. I think in the long run\n[01:01:08] >> a lot of people will choose that world.\n[01:01:09] I think some of yeah some people may\n[01:01:11] there's also I mean you know whether\n[01:01:14] you're consuming entertainment or\n[01:01:15] whether you're\n[01:01:17] doing something you know cooking or\n[01:01:19] painting or whatever because it's fun\n[01:01:21] and interesting to do what's missing\n[01:01:23] from that right all of that is purely\n[01:01:25] selfish\n[01:01:27] I think one of the reasons we work is\n[01:01:30] because we feel valued we feel like\n[01:01:33] we're benefiting other people\n[01:01:36] and I think some remember having this\n[01:01:39] conversation with um a lady in England\n[01:01:41] who helps to run the hospice movement.\n[01:01:45] And the people who work in the hospices\n[01:01:49] where you know the the patients are\n[01:01:50] literally there to die are largely\n[01:01:53] volunteers. So they're not doing it to\n[01:01:54] get paid\n[01:01:56] but they find it incredibly\n[01:01:59] rewarding to be able to spend time with\n[01:02:02] people who are in their last weeks or\n[01:02:05] months to give them company and\n[01:02:07] happiness.\n[01:02:09] So I actually think that interpersonal\n[01:02:14] roles\n[01:02:16] will be much much more important in\n[01:02:18] future. So if I was going to advise my\n[01:02:23] kids, not that they would ever listen,\n[01:02:24] but if I if my kids would listen and I\n[01:02:27] and and wanted to know what I thought\n[01:02:29] would be, you know, valued careers and\n[01:02:32] future, I think it would be these\n[01:02:34] interpersonal roles based on an\n[01:02:36] understanding of human needs,\n[01:02:37] psychology, there are some of those\n[01:02:39] roles right now. So obviously you know\n[01:02:43] therapists and psychiatrists and so on\n[01:02:45] but that that's a very much in sort of\n[01:02:47] asymmetric\n[01:02:50] role right where one person is suffering\n[01:02:52] and the other person is trying to\n[01:02:54] alleviate the suffering you know and\n[01:02:57] then there are things like they call\n[01:02:58] them executive coaches or life coaches\n[01:03:01] right that's a less asymmetric role\n[01:03:04] where someone is trying to uh help\n[01:03:08] another person live a better life\n[01:03:10] whether it's a better life in their work\n[01:03:12] role or or just uh how they live their\n[01:03:15] life in general. And so I could imagine\n[01:03:17] that those kinds of roles will expand\n[01:03:20] dramatically.\n[01:03:22] >> There's this interesting paradox that\n[01:03:24] exists when life becomes easier. Um\n[01:03:27] which shows that abundance consistently\n[01:03:30] pushes society societies towards more\n[01:03:34] individualism because once survival\n[01:03:36] pressures disappear, people prioritize\n[01:03:38] things differently. They prioritize\n[01:03:40] freedom, comfort, self-exression over\n[01:03:42] things like sacrifice or um family\n[01:03:45] formation. And we're seeing, I think, in\n[01:03:46] the west already, a decline in people\n[01:03:48] having kids because there's more\n[01:03:50] material abundance,\n[01:03:53] >> fewer kids, people are getting married\n[01:03:55] and committing to each other and having\n[01:03:57] relationships later and more\n[01:04:00] infrequently because generally once we\n[01:04:02] have more abundance, we don't want to\n[01:04:03] complicate our lives. Um, and at the\n[01:04:06] same time, as you said earlier, that\n[01:04:07] abundance breeds a an inability to find\n[01:04:11] meaning, a sort of shallowess to\n[01:04:13] everything. This is one of the things I\n[01:04:14] think a lot about, and I'm I'm in the\n[01:04:16] process now of writing a book about it,\n[01:04:17] which is this idea that individualism\n[01:04:20] was act is a bit of a lie. Like when I\n[01:04:22] say individualism and freedom, I mean\n[01:04:23] like the narrative at the moment amongst\n[01:04:25] my generation is you like be your own\n[01:04:27] boss and stand on your own two feet and\n[01:04:29] we're having less kids and we're not\n[01:04:31] getting married and it's all about me\n[01:04:33] me.\n[01:04:34] >> Yeah. That last part is where it goes\n[01:04:36] wrong.\n[01:04:36] >> Yeah. And it's like almost a\n[01:04:37] narcissistic society where\n[01:04:39] >> Yeah.\n[01:04:39] >> me me. My self-interest first. And when\n[01:04:42] you look at mental health outcomes and\n[01:04:44] loneliness and all these kinds of\n[01:04:45] things, it's going in a horrific\n[01:04:47] direction. But at the same time, we're\n[01:04:48] freer than ever. It seems like that you\n[01:04:51] know it seems like there's a we should\n[01:04:52] there's a maybe another story about\n[01:04:54] dependency which is not sexy like depend\n[01:04:56] on each other.\n[01:04:57] >> Oh I I I agree. I mean I think you know\n[01:05:00] happiness is not available from\n[01:05:03] consumption or even lifestyle right I\n[01:05:06] think happiness\n[01:05:08] arises from giving. [snorts]\n[01:05:12] It can be you through the work that you\n[01:05:15] do, you can see that other people\n[01:05:17] benefit from that or it could be in\n[01:05:19] direct interpersonal relationships.\n[01:05:22] >> There is an invisible tax on salespeople\n[01:05:24] that no one really talks about enough.\n[01:05:26] The mental load of remembering\n[01:05:27] everything like meeting notes,\n[01:05:29] timelines, and everything in between\n[01:05:31] until we started using our sponsors\n[01:05:33] product called Pipe Drive, one of the\n[01:05:34] best CRM tools for small and\n[01:05:36] medium-sized business owners. The idea\n[01:05:38] here was that it might alleviate some of\n[01:05:40] the unnecessary cognitive overload that\n[01:05:42] my team was carrying so that they could\n[01:05:44] spend less time in the weeds of admin\n[01:05:46] and more time with clients, in-person\n[01:05:48] meetings, and building relationships.\n[01:05:49] Pipe Drive has enabled this to happen.\n[01:05:51] It's such a simple but effective CRM\n[01:05:54] that automates the tedious, repetitive,\n[01:05:57] and timeconuming parts of the sales\n[01:05:58] process. And now our team can nurture\n[01:06:01] those leads and still have bandwidth to\n[01:06:03] focus on the higher priority tasks that\n[01:06:05] actually get the deal over the line.\n[01:06:06] Over a 100,000 companies across 170\n[01:06:09] countries already use Pipe Drive to grow\n[01:06:11] their business. And I've been using it\n[01:06:12] for almost a decade now. Try it free for\n[01:06:15] 30 days. No credit card needed, no\n[01:06:17] payment needed. Just use my link\n[01:06:19] pipedive.com/ceo\n[01:06:22] to get started today. That's\n[01:06:24] pipedive.com/ceo.\n[01:06:27] Where does the rewards of this AI race\n[01:06:31] where does it acrue to?\n[01:06:34] I think a lot about this in terms of\n[01:06:35] like univers universal basic income. If\n[01:06:37] you have these five, six, seven, 10\n[01:06:40] massive AI companies that are going to\n[01:06:42] win the 15 quadrillion dollar prize.\n[01:06:46] >> Mhm.\n[01:06:46] >> And they're going to automate all of the\n[01:06:48] professional pursuits that we we\n[01:06:50] currently have. All of our jobs are\n[01:06:52] going to go away.\n[01:06:54] Who who gets all the money? And how do\n[01:06:56] how do we get some of it back?\n[01:06:57] [laughter]\n[01:06:58] >> Money actually doesn't matter, right?\n[01:06:59] what what matters is the production of\n[01:07:02] goods and services uh and then how those\n[01:07:06] are distributed and so so money acts as\n[01:07:09] a way to facilitate the distribution and\n[01:07:12] um exchange of those goods and services.\n[01:07:14] If all production is concentrated\n[01:07:17] um in the hands of a of a few companies,\n[01:07:21] right, that\n[01:07:22] sure they will lease some of their\n[01:07:25] robots to us. You know, we we want a\n[01:07:27] school in our village.\n[01:07:30] They lease the robots to us. The robots\n[01:07:32] build the school. They go away. We have\n[01:07:34] to pay a certain amount of of money for\n[01:07:36] that. But where do we get the money?\n[01:07:39] Right? If we are not producing anything\n[01:07:43] then uh we don't have any money unless\n[01:07:46] there's some redistribution mechanism.\n[01:07:48] And as you mentioned, so universal basic\n[01:07:50] income is\n[01:07:53] it seems to me an admission of failure\n[01:07:57] because what it says is okay, we're just\n[01:07:58] going to give everyone the money and\n[01:08:00] then they can use the money to pay the\n[01:08:02] AI company to lease the robots to build\n[01:08:04] the school and then we'll have a school\n[01:08:06] and that's good. Um\n[01:08:09] but what it's an admission of failure\n[01:08:12] because it says we can't work out a\n[01:08:14] system in which people have any worth or\n[01:08:18] any economic role.\n[01:08:21] Right? So 99% of the global population\n[01:08:24] is\n[01:08:25] from an economic point of view useless.\n[01:08:28] Can I ask you a question? If you had a\n[01:08:30] button in front of you and pressing that\n[01:08:33] button would stop all progress in\n[01:08:36] artificial intelligence right now and\n[01:08:38] forever, would you press it?\n[01:08:40] >> That's a very interesting question. Um,\n[01:08:45] if it's either or\n[01:08:48] either I do it now or it's too late and\n[01:08:51] we\n[01:08:53] careen into some uncontrollable future\n[01:08:57] perhaps. Yeah, cuz I I'm not super\n[01:09:01] optimistic that we're heading in the\n[01:09:02] right direction at all.\n[01:09:03] >> So, I put that button in front of you\n[01:09:04] now. It stops all AI progress, shuts\n[01:09:06] down all the AI companies immediately\n[01:09:08] globally, and none of them can reopen.\n[01:09:10] You press it.\n[01:09:17] Well, here's here's what I think should\n[01:09:19] happen. So, obviously, you know, I've\n[01:09:22] been doing AI for 50 years. um and\n[01:09:27] the original motivations which is that\n[01:09:30] AI can be a power tool for humanity\n[01:09:33] enabling us to do\n[01:09:36] more and better things than we can\n[01:09:38] unaded. I think that's still valid. The\n[01:09:42] problem is\n[01:09:44] the kinds of AI systems that we're\n[01:09:45] building are not tools. They are\n[01:09:47] replacements. In fact, you can see this\n[01:09:50] very clearly because we create them\n[01:09:53] literally as the closest replicas we can\n[01:09:57] make of human beings.\n[01:10:00] The technique for creating them is\n[01:10:03] called imitation learning. So we observe\n[01:10:07] human verbal behavior, writing or\n[01:10:09] speaking and we make a system that\n[01:10:12] imitates that as well as possible.\n[01:10:17] So what we are making is imitation\n[01:10:18] humans at least in the verbal sphere.\n[01:10:23] And so of course they're going to\n[01:10:24] replace us.\n[01:10:27] They're not tools.\n[01:10:28] >> So you had pressed the button.\n[01:10:30] >> So I say I think there is another course\n[01:10:34] which is use and develop AI as tools.\n[01:10:38] Tools for science\n[01:10:41] tools for economic organization and so\n[01:10:43] on.\n[01:10:44] um but not as replacements for human\n[01:10:49] beings.\n[01:10:49] >> What I like about this question is it\n[01:10:51] forces you to go into the pro into\n[01:10:53] probabilities.\n[01:10:54] >> Yeah. So, and that's that's why I'm\n[01:10:57] reluctant because I don't I don't agree\n[01:11:01] with the, you know, what's your\n[01:11:02] probability of doom,\n[01:11:03] >> right? Your so-called P of doom uh\n[01:11:06] number because that makes sense if\n[01:11:08] you're an alien.\n[01:11:10] You know, you're in you're in a bar with\n[01:11:12] some other aliens and you're looking\n[01:11:13] down at the Earth and you're taking bets\n[01:11:15] on, you know, are these humans going to\n[01:11:16] make a mess of things and go extinct\n[01:11:18] because they develop AI.\n[01:11:21] So, it's fine for those aliens to bet on\n[01:11:24] on that, but if you're a human, then\n[01:11:27] you're not just betting, you're actually\n[01:11:29] acting.\n[01:11:30] >> There there's an element to this though,\n[01:11:32] which I guess where probabilities do\n[01:11:33] come back in, which is you also have to\n[01:11:35] weigh when I give you such a binary\n[01:11:37] decision.\n[01:11:40] um the probability of us pursuing the\n[01:11:43] more nuanced safe approach into that\n[01:11:46] equation. So you're you're the the maths\n[01:11:49] in my head is okay, you've got all the\n[01:11:50] upsides here and then you've got\n[01:11:52] potential downsides and then there's a\n[01:11:54] probability of do I think we're actually\n[01:11:56] going to course correct based on\n[01:11:57] everything I know based on the incentive\n[01:11:59] structure of human beings and and\n[01:12:00] countries and then if there's but then\n[01:12:03] you could go if there's even a 1%\n[01:12:06] chance of extinction\n[01:12:09] is it even worth all these upsides?\n[01:12:11] >> Yeah. And I I would argue no. I mean\n[01:12:14] maybe maybe what we would say if if we\n[01:12:16] said okay it's going to stop the\n[01:12:18] progress for 50 years\n[01:12:19] >> you press it\n[01:12:20] >> and during those 50 years we can work on\n[01:12:23] how do we do AI in a way that's\n[01:12:25] guaranteed to be safe and beneficial how\n[01:12:28] do we organize\n[01:12:30] our societies to flourish uh in\n[01:12:33] conjunction with extremely capable AI\n[01:12:36] systems. So, we haven't answered either\n[01:12:38] of those questions.\n[01:12:39] And I don't think we want anything\n[01:12:42] resembling AGI until we have completely\n[01:12:45] solid answers to both of those\n[01:12:46] questions. So, if there was a button\n[01:12:48] where I could say, \"All right, we're\n[01:12:49] going to pause progress for 50 years.\"\n[01:12:52] Yes, I would do it.\n[01:12:53] >> But if that button was in front of you,\n[01:12:54] you're going to make a decision either\n[01:12:55] way. Either you don't press it or you\n[01:12:57] press it.\n[01:12:57] >> I If Yeah. So, if that if that button is\n[01:13:00] there, stop it for 50 years. I would say\n[01:13:02] yes.\n[01:13:05] stop it forever?\n[01:13:09] Not yet. I think I think there's still a\n[01:13:13] decent chance that we can pull out of\n[01:13:16] this uh nose dive, so to speak, that\n[01:13:18] we're we're currently in. Ask me again\n[01:13:21] in a year, I might I might say, \"Okay,\n[01:13:24] we do need to press the button.\"\n[01:13:25] >> What if What if in a scenario where you\n[01:13:27] never get to reverse that decision? You\n[01:13:29] never get to make that decision again.\n[01:13:30] So if in that scenario that I've laid\n[01:13:32] out this hypothetical, you either press\n[01:13:34] it now or it never gets pressed.\n[01:13:37] So there is no opportunity a year from\n[01:13:38] now.\n[01:13:41] >> Yeah, as you can tell, I'm [laughter]\n[01:13:43] sort of on on the fence a bit about\n[01:13:46] about this one. Um\n[01:13:49] yeah, I think I'd probably press it.\n[01:13:52] Yeah.\n[01:13:55] >> What's your reasoning?\n[01:13:58] uh just thinking about the power\n[01:14:00] dynamics\n[01:14:02] of um\n[01:14:04] what's happening now how difficult would\n[01:14:07] it would be to get the US in particular\n[01:14:09] to to regulate in favor of safety.\n[01:14:14] So I think you know what's clear from\n[01:14:15] talking to the companies is they are not\n[01:14:18] going to develop anything resembling\n[01:14:23] safe AGI unless they're forced to by the\n[01:14:26] government.\n[01:14:27] And at the moment the US government in\n[01:14:30] particular which regulates most of the\n[01:14:32] leading companies in AI is not only\n[01:14:36] refusing to regulate but even trying to\n[01:14:39] prevent the states from regulating. And\n[01:14:42] they're doing that at the behest of\n[01:14:46] uh a faction within Silicon Valley uh\n[01:14:50] called the accelerationists\n[01:14:52] who believe that the faster we get to\n[01:14:55] AGI the better. And when I say behest I\n[01:14:58] mean also they paid them a large amount\n[01:15:00] of money. Jensen Hang the the CEO of\n[01:15:02] Nvidia said who is for anyone that\n[01:15:04] doesn't know the guy making all the\n[01:15:06] chips that are powering AI said China is\n[01:15:08] going to win the AI race arguing it is\n[01:15:11] just a nanocond behind the United\n[01:15:13] States. China have produced 24,000 AI\n[01:15:17] papers compared to just 6,000\n[01:15:21] from the US\n[01:15:23] more than the combined output of the US\n[01:15:25] the UK and the EU.\n[01:15:27] China is anticipated to quickly roll out\n[01:15:29] their new technologies both domestically\n[01:15:31] and developing new technologies for\n[01:15:33] other developing countries.\n[01:15:36] So the accelerators or the accelerate I\n[01:15:38] think you call them the accelerants\n[01:15:40] >> accelerationists.\n[01:15:41] >> The accelerationists\n[01:15:42] >> I mean they would say well if we don't\n[01:15:44] then China will. So we have to we have\n[01:15:46] to go fast. It's another version of the\n[01:15:48] the race that the companies are in with\n[01:15:50] each other, right? That we, you know, we\n[01:15:52] know that this race is\n[01:15:54] heading off a cliff,\n[01:15:57] but we can't stop. So, we're all just\n[01:16:00] going to go off this cliff. And\n[01:16:02] obviously, that's nuts,\n[01:16:04] right? I mean, we're all looking at each\n[01:16:05] other saying, \"Yeah, there's a cliff\n[01:16:06] over there.\" Running as fast as we can\n[01:16:08] towards this cliff. We're looking at\n[01:16:10] each other saying, \"Why aren't we\n[01:16:11] stopping?\"\n[01:16:13] So the narrative in Washington, which I\n[01:16:16] think Jensen Hang is\n[01:16:19] either reflecting or or perhaps um\n[01:16:21] promoting\n[01:16:23] uh is that you know, China has is\n[01:16:28] completely unregulated\n[01:16:30] and uh you know, America will only slow\n[01:16:32] itself down uh if it regulates a AI in\n[01:16:36] any way. So this is a completely false\n[01:16:38] narrative because China's AI regulations\n[01:16:42] are actually quite strict even compared\n[01:16:44] to um the European Union\n[01:16:48] and China's government has explicitly\n[01:16:51] acknowledged uh the need and their\n[01:16:54] regulations are very clear. You can't\n[01:16:56] build AI systems that could escape human\n[01:16:58] control. And not only that, I don't\n[01:17:01] think they view the race in the same way\n[01:17:04] as, okay, we we just need to be the\n[01:17:07] first to create AGI. I think they're\n[01:17:11] more interested in figuring out how to\n[01:17:15] disseminate AI as a set of tools within\n[01:17:19] their economy to make their economy more\n[01:17:21] productive and and so on. So that's\n[01:17:23] that's their version of the race.\n[01:17:25] >> But of course, they still want to build\n[01:17:26] the weapons for adversaries, right? to\n[01:17:28] so that they can take down I don't know\n[01:17:32] Taiwan if they want to.\n[01:17:34] >> So weapons are a separate matter and I\n[01:17:36] happy to talk about weapons but just in\n[01:17:37] terms of\n[01:17:38] >> control\n[01:17:39] >> uh control economic domination\n[01:17:42] um they they don't view putting all your\n[01:17:46] eggs in the AGI basket as the right\n[01:17:49] strategy. So they want to use AI, you\n[01:17:53] know, even in its present form to make\n[01:17:55] their economy much more efficient and\n[01:17:57] productive and also, you know, to give\n[01:18:01] people new capabilities and and better\n[01:18:04] quality of life and and I think the US\n[01:18:07] could do that as well. And\n[01:18:11] um typically western countries don't\n[01:18:14] have as much of uh central government\n[01:18:17] control over what companies do and some\n[01:18:20] companies are investing in AI to make\n[01:18:22] their operations more efficient uh and\n[01:18:26] some are not and we'll see how that\n[01:18:27] plays out.\n[01:18:28] >> What do you think of Trump's approach to\n[01:18:29] AI? So Trump's approach is, you know,\n[01:18:31] it's it's echoing what Jensen Wang is\n[01:18:33] saying that the US has to be the one to\n[01:18:35] create AGI and very explicitly the\n[01:18:39] administration's policy is to uh\n[01:18:42] dominate the world.\n[01:18:44] That's the word they use, dominate. I'm\n[01:18:46] not sure that other countries like the\n[01:18:49] idea that um they will be dominated by\n[01:18:52] American AI. But is that an accurate\n[01:18:55] description of what will happen if the\n[01:18:56] US build AGI technology before say the\n[01:18:59] UK where I'm originally from and where\n[01:19:01] you're originally from? What does the\n[01:19:04] This is something I think about a lot\n[01:19:05] because we're going through this budget\n[01:19:06] process in the UK at the moment where\n[01:19:07] we're figuring out how we going to spend\n[01:19:08] our money and how we're going to tax\n[01:19:09] people and also we've got this new\n[01:19:11] election cycle. It's approaching quickly\n[01:19:14] where people are talking about\n[01:19:15] immigration issues and this issue and\n[01:19:17] that issue and the other issue. What I\n[01:19:18] don't hear anyone talking about is AI\n[01:19:21] and the humanoid robots that are\n[01:19:23] going to take everything. We're very\n[01:19:24] concerned with the brown people crossing\n[01:19:25] the channel, but the humanoid robots\n[01:19:27] that are going to be super intelligent\n[01:19:29] and really take causing economic disrupt\n[01:19:32] disruption. No one talks about that. The\n[01:19:33] political leaders don't talk about it.\n[01:19:35] It doesn't win races. I don't see it on\n[01:19:36] billboards.\n[01:19:37] >> Yeah. And it's it it's interesting\n[01:19:39] because\n[01:19:41] in fact I mean so there's there's two\n[01:19:43] forces that have been hollowing out the\n[01:19:45] middle classes in western countries. One\n[01:19:49] of them is globalization where lots and\n[01:19:52] lots of work not just manufacturing but\n[01:19:54] white collar work gets outsourced to\n[01:19:56] low-income countries. Uh but the other\n[01:19:58] is automation\n[01:20:01] and you know some of that is factories.\n[01:20:03] So um the amount of employment in\n[01:20:07] manufacturing continues to drop even as\n[01:20:10] the amount of output from manufacturing\n[01:20:13] in the US and in the UK continues to\n[01:20:15] increase. So we talk about oh you know\n[01:20:17] our manufacturing industry has been\n[01:20:19] destroyed. It hasn't. It's producing\n[01:20:21] more than ever just with you know a\n[01:20:24] quarter as many people. So it's\n[01:20:26] manufacturing employment that's been\n[01:20:27] destroyed by automation and robotics and\n[01:20:31] so on. And then you know computerization\n[01:20:34] has eliminated whole layers of white\n[01:20:37] collar jobs. And so those two those two\n[01:20:41] forms of automation have probably done\n[01:20:44] more to hollow out middle class uh\n[01:20:47] employment and standard of life.\n[01:20:50] >> If the UK doesn't participate\n[01:20:53] in this new e technological wave\n[01:20:57] that seems to be that seems to you know\n[01:20:59] it's going to take a lot of jobs. cars\n[01:21:00] are going to drive themselves. Whimo\n[01:21:02] just announced that they're coming to\n[01:21:03] London, which is the driverless cars,\n[01:21:05] and driving is the biggest occupation in\n[01:21:07] the world, for example. So, you've got\n[01:21:08] immediate disruption there. And where\n[01:21:10] does the money acrew to? Well, it acrus\n[01:21:11] to who owns Whimo, which is what? Google\n[01:21:14] and Silicon Valley companies.\n[01:21:16] >> Alphabet owns Whimo 100%. I think so.\n[01:21:18] Yes. I mean this is so I was in India a\n[01:21:20] few months ago talking to the government\n[01:21:23] ministers because they're holding the\n[01:21:24] next global AI summit in February and\n[01:21:28] and their view going in was you know AI\n[01:21:32] is great we're going to use it to you\n[01:21:34] know turbocharge the growth of our\n[01:21:36] Indian economy\n[01:21:38] when for example you have AGI you have\n[01:21:41] AGI controlled robots\n[01:21:44] that can do all the manufacturing that\n[01:21:45] can do agriculture that can do all the\n[01:21:48] white work and goods and services that\n[01:21:51] might have been produced by Indians will\n[01:21:54] instead be produced by\n[01:21:57] American controlled\n[01:22:00] AGI systems at much lower prices. You\n[01:22:04] know, a consumer given a choice between\n[01:22:06] an expensive product produced by Indians\n[01:22:08] or a cheap product produced by American\n[01:22:10] robots will probably choose\n[01:22:14] the cheap product produced by American\n[01:22:15] robots. And so potentially every country\n[01:22:18] in the world with the possible exception\n[01:22:20] of North Korea will become a kind of a\n[01:22:22] client state\n[01:22:25] of American AI companies.\n[01:22:28] >> A client state of American AI companies\n[01:22:30] is exactly what I'm concerned about for\n[01:22:32] the UK economy. Really any economy\n[01:22:34] outside of the United States. I guess\n[01:22:36] one could also say China, but because\n[01:22:39] those are the two nations that are\n[01:22:40] taking AI most seriously.\n[01:22:42] >> Mhm. [clears throat]\n[01:22:43] >> And I I I don't know what our economy\n[01:22:45] becomes. cuz I can't figure out\n[01:22:48] can't figure out what our what the\n[01:22:49] British economy becomes in such a world.\n[01:22:52] Is it tourism? I don't know. Like you\n[01:22:53] come here to to to look at the\n[01:22:55] Buckingham Palace. I\n[01:22:56] >> you you can think about countries but I\n[01:22:58] mean even for the United States it's the\n[01:23:00] same problem.\n[01:23:01] >> At least they'll be able to hell out you\n[01:23:03] know. So some small fraction of the\n[01:23:05] population will be running maybe the AI\n[01:23:09] companies but increasingly\n[01:23:12] even those companies will be replacing\n[01:23:14] their human employees with AI systems.\n[01:23:18] >> So [clears throat] Amazon for example\n[01:23:19] which you know sells a lot of computing\n[01:23:22] services to AI companies is using AI to\n[01:23:24] replace layers of management is planning\n[01:23:27] to use robots to replace all of its\n[01:23:29] warehouse workers and so on. So, so even\n[01:23:33] the the giant AI companies\n[01:23:36] will have few human employees in the\n[01:23:39] long run. I mean, it think of the\n[01:23:42] situation, you know, pity the poor CEO\n[01:23:44] whose board\n[01:23:46] says, \"Well, you know, unless you turn\n[01:23:49] over your decision-making power to the\n[01:23:50] AI system, um, we're going to have to\n[01:23:53] fire you because all our competitors are\n[01:23:56] using, you know, an AI powered CEO and\n[01:24:00] they're doing much better.\" Amazon plans\n[01:24:01] to replace 600,000 workers with robots\n[01:24:04] in a memo that just leaked, which has\n[01:24:06] been widely talked about. And the CEO,\n[01:24:08] Andy Jasse, told employees that the\n[01:24:10] company expects its corporate workforce\n[01:24:12] to shrink in the coming years because of\n[01:24:14] AI and AI agents. And they've publicly\n[01:24:17] gone live with saying that they're going\n[01:24:18] to cut 14,000 corporate jobs in the near\n[01:24:21] term as part of its refocus on AI\n[01:24:25] investment and efficiency.\n[01:24:28] It's interesting because I was reading\n[01:24:29] about um the sort of different quotes\n[01:24:32] from different AI leaders about the\n[01:24:33] speed in which this this stuff is going\n[01:24:35] to happen [snorts] and what you see in\n[01:24:37] the quotes is Demis who's the CEO of\n[01:24:40] DeepMind\n[01:24:41] >> saying things like it'll be more than 10\n[01:24:44] times bigger than the industrial\n[01:24:45] revolution but also it'll happen maybe\n[01:24:47] 10 times faster and they speak about\n[01:24:50] this turbulence that we're going to\n[01:24:52] experience as this shift takes place.\n[01:24:55] That's um maybe a euphemism\n[01:24:58] for [laughter]\n[01:24:59] uh and I think that you know governments\n[01:25:00] are now\n[01:25:02] you know they they've kind of gone from\n[01:25:04] saying oh don't worry you know we'll\n[01:25:05] just retrain everyone as data scientists\n[01:25:07] like well yeah that's that's ridiculous\n[01:25:09] right the world doesn't need four\n[01:25:10] billion data scientists\n[01:25:11] >> and we're not all capable of becoming\n[01:25:13] that by the way\n[01:25:14] >> uh yeah or have any interest in in doing\n[01:25:17] that\n[01:25:17] >> I I could even if I wanted to like I\n[01:25:19] tried to sit in biology class and I fell\n[01:25:20] asleep so [laughter] I couldn't that was\n[01:25:22] the end of my career as a surgeon. Fair\n[01:25:24] enough. Um, [clears throat]\n[01:25:26] but yeah, now suddenly they're staring,\n[01:25:28] you know, 80% unemployment in the face\n[01:25:31] and wondering how how on earth is our\n[01:25:34] society going to hold together.\n[01:25:36] >> We'll deal with it when we get there.\n[01:25:38] >> Yeah. Unfortunately, um,\n[01:25:41] unless we plan ahead,\n[01:25:45] we're going to suffer the consequences,\n[01:25:46] right? can't. It was bad enough in the\n[01:25:48] industrial revolution which unfolded\n[01:25:50] over seven or eight decades but there\n[01:25:53] was massive disruption\n[01:25:56] and uh misery\n[01:25:59] caused by that. We don't have a model\n[01:26:01] for a functioning society where almost\n[01:26:05] everyone does nothing\n[01:26:08] at least nothing of economic value.\n[01:26:11] Now, it's not impossible that there\n[01:26:13] could be such a a functioning society,\n[01:26:15] but we don't know what it looks like.\n[01:26:17] And you know, when you think about our\n[01:26:19] education system, which would probably\n[01:26:22] have to look very different and how long\n[01:26:24] it takes to change that. I mean, I'm\n[01:26:26] always\n[01:26:27] reminding people about uh how long it\n[01:26:30] took Oxford to decide that geography was\n[01:26:33] a proper subject of study. It took them\n[01:26:36] 125 years from the first proposal that\n[01:26:39] there should be a geography degree until\n[01:26:41] it was finally approved. So we don't\n[01:26:43] have very long\n[01:26:47] to completely revamp a system that we\n[01:26:51] know takes decades and decades\n[01:26:54] to reform and we don't know how to\n[01:26:58] reform it because we don't know what we\n[01:27:01] want the world to look like. Is this one\n[01:27:03] of your reasons why you're appalled at\n[01:27:07] the moment? Because when you have these\n[01:27:08] conversations with people, people just\n[01:27:10] don't have answers, yet they're plowing\n[01:27:12] ahead at rapid speed.\n[01:27:13] >> I would say it's not necessarily the job\n[01:27:16] of the AI companies. So, I'm appalled by\n[01:27:18] the AI companies because they don't have\n[01:27:20] an answer for how they're going to\n[01:27:21] control the systems that they're\n[01:27:22] proposing to build. I do find it\n[01:27:26] disappointing that uh governments don't\n[01:27:29] seem to be grappling with this issue. I\n[01:27:32] think there are a few I think for\n[01:27:33] example Singapore government seems to be\n[01:27:35] quite farsighted and they've they've\n[01:27:38] thought this through you know it's a\n[01:27:40] small country they've figured out okay\n[01:27:42] this this will be our role uh going\n[01:27:44] forward and we think we can find you\n[01:27:47] know some some purpose for our people in\n[01:27:49] this in this new world but for I think\n[01:27:51] countries with large populations\n[01:27:54] um [clears throat]\n[01:27:56] they need to figure out answers to these\n[01:27:59] questions pretty fast it takes a long\n[01:28:01] time to implement those answers uh in\n[01:28:04] the form of new kinds of education, new\n[01:28:07] professions, new qualifications,\n[01:28:10] uh new economic structures.\n[01:28:13] I mean, it's it's it's possible. I mean,\n[01:28:16] when you look at therapists, for\n[01:28:17] example, they're almost all\n[01:28:19] self-employed.\n[01:28:22] So, what happens when, you know, 80% of\n[01:28:25] the population transitions from regular\n[01:28:28] employment into into self-employment?\n[01:28:31] what does that what does that do to the\n[01:28:32] economics of of uh government finances\n[01:28:36] and so on. So there's just lots of\n[01:28:38] questions and how do you you know if\n[01:28:40] that's the future you know why are we\n[01:28:42] training people to to fit into 9 to5\n[01:28:45] office jobs which won't exist at all\n[01:28:48] >> last month I told you about a challenge\n[01:28:50] that I'd set our internal flightex team\n[01:28:52] flight team is our innovation team\n[01:28:53] internally here I tasked them with\n[01:28:55] seeing how much time they could unlock\n[01:28:57] for the company by creating something\n[01:28:59] that would help us filter new AI tools\n[01:29:01] to see which ones were worth pursuing\n[01:29:03] and I thought that our sponsor Fiverr\n[01:29:05] Pro might have the talent on their\n[01:29:07] platform to help us build this quickly.\n[01:29:09] So I talked to my director of innovation\n[01:29:11] Isaac and for the last month my team\n[01:29:13] Flight X and a vetted [clears throat] AI\n[01:29:15] specialist from Fiverr Pro have been\n[01:29:17] working together on this project and\n[01:29:19] with the help of my team we've been able\n[01:29:20] to create a brand new tool which\n[01:29:22] automatically scans scores and\n[01:29:24] prioritizes different emerging AI tools\n[01:29:26] for us. Its impact has been huge and\n[01:29:29] within a couple of weeks this tool has\n[01:29:31] already been saving us hours triing and\n[01:29:32] testing new AI systems. Instead of\n[01:29:35] shifting through lots of noise, my team\n[01:29:36] flight X has been able to focus on\n[01:29:38] developing even more AI tools, ones that\n[01:29:40] really move the needle in our business\n[01:29:42] thanks to the talent on Fiverr Pro. So,\n[01:29:44] if you've got a complex problem and you\n[01:29:47] need help solving it, make sure you\n[01:29:48] check out Fiverr Pro at\n[01:29:50] fiverr.com/diary.\n[01:29:53] So, many of us are pursuing passive\n[01:29:55] forms of income and to build side\n[01:29:57] businesses in order to help us cover our\n[01:29:59] bills. And that opportunity is here with\n[01:30:01] our sponsor Stan, a business that I\n[01:30:03] co-own. It is the platform that can help\n[01:30:05] you take full advantage of your own\n[01:30:08] financial situation. Stan enables you to\n[01:30:10] work for yourself. It makes selling\n[01:30:12] digital products, courses, memberships,\n[01:30:14] and more simple products more scalable\n[01:30:16] and easier to do. You can turn your\n[01:30:18] ideas into income and get the support to\n[01:30:20] grow whatever you're building. And we're\n[01:30:22] about to launch Dare to Dream. It's for\n[01:30:25] those who are ready to make the shift\n[01:30:26] from thinking to building, from planning\n[01:30:29] to actually doing the thing. It's about\n[01:30:31] seeing that dream in your head and\n[01:30:32] knowing exactly what it takes to bring\n[01:30:34] it to life. If you're ready to transform\n[01:30:36] your life, visit daretodream.stan.store.\n[01:30:41] You've made many attempts to raise\n[01:30:43] awareness and to call for a heightened\n[01:30:46] consciousness about the future of AI.\n[01:30:49] Um, in October, over 850 experts,\n[01:30:52] including yourself and other leaders,\n[01:30:53] like Richard Branson, who I've had on\n[01:30:55] the show, and Jeffrey Hinton, who I've\n[01:30:56] had on the show, signed a statement to\n[01:30:58] ban AI super intelligence, as you guys\n[01:31:01] raised concerns of potential human\n[01:31:03] extinction.\n[01:31:04] >> Sort of. Yeah. It says, at least until\n[01:31:07] we are sure that we can move forward\n[01:31:08] safely and there's broad scientific\n[01:31:10] consensus on that. So, that\n[01:31:13] >> did it work? [gasps]\n[01:31:15] >> It's hard. It's hard to say. I mean\n[01:31:17] interestingly there was a related so\n[01:31:19] what was called the the pause statement\n[01:31:21] was March of 23. So that was when GPT4\n[01:31:25] came out the successor to chat GPT. So\n[01:31:29] we we suggested that there'd be a\n[01:31:30] six-month pause in developing and\n[01:31:33] deploying systems more powerful than\n[01:31:35] GPD4. And everyone poo pooed that idea.\n[01:31:39] Of course no one's going to pause\n[01:31:40] anything. But in fact, there were no\n[01:31:41] systems in the next 6 months deployed\n[01:31:44] that were more powerful than GPT4.\n[01:31:47] Um, none coincidence. You be the judge.\n[01:31:50] I would say\n[01:31:52] that what we're trying to do is to is to\n[01:31:56] basically shift\n[01:31:58] the\n[01:31:59] the public debate.\n[01:32:01] You know there's this bizarre phenomenon\n[01:32:04] that keeps happening in the media\n[01:32:07] where if you talk about these risks\n[01:32:11] they will say oh you know there's a\n[01:32:13] fringe of people you know called quote\n[01:32:16] doomers who think that there's you know\n[01:32:18] risk of extinction. Um so they always\n[01:32:22] the narrative is always that oh you know\n[01:32:24] talking about those risk is a fringe\n[01:32:25] thing. Pretty much all the CEOs of the\n[01:32:28] leading AI companies\n[01:32:30] think that there's a significant risk of\n[01:32:32] extinction. Almost all the leading AI\n[01:32:35] researchers think there's a sign\n[01:32:36] significant risk of human extinction.\n[01:32:39] Um so\n[01:32:42] why is that the fringe, right? Why isn't\n[01:32:43] that the mainstream? If the these are\n[01:32:45] the leading experts in industry and\n[01:32:47] academia\n[01:32:49] uh saying this, how could it be the\n[01:32:51] fringe? So we're trying to change that\n[01:32:54] narrative\n[01:32:55] to say no, the people who really\n[01:32:58] understand this stuff are extremely\n[01:33:01] concerned.\n[01:33:03] >> And what do you want to happen? What is\n[01:33:05] the solution?\n[01:33:06] >> What I think is that we should have\n[01:33:08] effective regulation.\n[01:33:11] It's hard to argue with that, right? Uh\n[01:33:13] so what does effective mean? It means\n[01:33:15] that if you comply with the regulation,\n[01:33:18] then the risks are reduced to an\n[01:33:20] acceptable level.\n[01:33:23] So for example,\n[01:33:26] we ask people who want to operate\n[01:33:28] nuclear plants, right? We've decided\n[01:33:31] that the risk we're willing to live with\n[01:33:33] is, you know, a one in a million chance\n[01:33:37] per year that the plant is going to have\n[01:33:39] a meltdown. Any higher than that, you\n[01:33:42] know, we just don't it's not worth it.\n[01:33:44] Right. So you have to be below that.\n[01:33:46] Some cases we can get down to one in 10\n[01:33:49] million chance per year. So what chance\n[01:33:52] do you think we should be willing to\n[01:33:53] live with for human extinction?\n[01:33:57] >> Me?\n[01:33:58] >> Yeah.\n[01:34:01] [clears throat]\n[01:34:02] >> 0.00001.\n[01:34:04] >> Yeah. Lots of zeros.\n[01:34:05] >> Yeah.\n[01:34:06] >> Right. So one in a million for a nuclear\n[01:34:09] meltdown.\n[01:34:11] >> Extinction is much worse.\n[01:34:12] >> Oh yeah. So yeah, it's kind of right. So\n[01:34:14] >> one in 100 billion, one in a trillion.\n[01:34:16] >> Yeah. So if you said one in a billion,\n[01:34:18] right, then you'd expect one extinction\n[01:34:20] per billion years. There's a background.\n[01:34:23] So one one of the ways people work out\n[01:34:25] these risk levels is also to look at the\n[01:34:26] background. The other ways of getting\n[01:34:29] going extinct would include, you know,\n[01:34:30] giant asteroid crashes into the earth.\n[01:34:32] And you can roughly calculate what those\n[01:34:35] probabilities are. We can look at how\n[01:34:36] many extinction level events have\n[01:34:39] happened in the past and, you know,\n[01:34:40] maybe it's half a dozen over. So, so\n[01:34:42] there's maybe it's like a one in 500\n[01:34:45] million year event. So, somewhere in\n[01:34:49] that range, right? Somewhere between 1\n[01:34:51] in 10 million, which is the best nuclear\n[01:34:53] power plants, and and one in 500 million\n[01:34:55] or one in a billion, which is the\n[01:34:58] background\n[01:34:59] risk from from giant asteroids. Uh so,\n[01:35:02] let's say we settle on 100 million, one\n[01:35:04] in a 100 million chance per year. Well,\n[01:35:06] what is it according to the CEOs? 25%.\n[01:35:11] So they're off by a factor of multiple\n[01:35:16] millions,\n[01:35:18] right? So they need to make the AI\n[01:35:20] systems millions of times safer.\n[01:35:23] >> Your analogy of the roulette, Russian\n[01:35:25] roulette comes back in here because\n[01:35:27] that's like for anyone that doesn't know\n[01:35:28] what probabilities are in this context,\n[01:35:30] that's like having a ammunition chamber\n[01:35:34] with four holes in it and putting a\n[01:35:37] bullet in one of them.\n[01:35:38] >> One in four. [clears throat] Yeah. And\n[01:35:39] we're saying we want it to be one in a\n[01:35:40] billion. So we want a billion chambers\n[01:35:43] and a bullet in one of them.\n[01:35:44] >> Yeah. And and so when you look at the\n[01:35:47] work that the nuclear operators have to\n[01:35:48] do to show that their system is that\n[01:35:51] reliable,\n[01:35:53] uh it's a massive mathematical analysis\n[01:35:56] of the components, you know, redundancy.\n[01:35:59] You've got monitors, you've got warning\n[01:36:01] lights, you've got operating procedures.\n[01:36:04] You have all kinds of mechanisms which\n[01:36:07] over the decades have ratcheted that\n[01:36:09] risk down. It started out I think one in\n[01:36:12] one in 10,000 years, right? And they've\n[01:36:15] improved it by a factor of 100 or a\n[01:36:17] thousand by all of these mechanisms. But\n[01:36:20] at every stage they had to do a\n[01:36:21] mathematical analysis to show what the\n[01:36:23] risk was.\n[01:36:26] The people developing the AI company,\n[01:36:28] the AI systems, sorry, the AI companies\n[01:36:30] developing these systems, they don't\n[01:36:32] even understand how the AI systems work.\n[01:36:34] So their 25% chance of extinction is\n[01:36:37] just a seat of the pants guess. They\n[01:36:39] actually have no idea.\n[01:36:41] But the tests that they are doing on\n[01:36:44] their systems right now, you know, they\n[01:36:46] show that the AI systems will be willing\n[01:36:49] to kill people\n[01:36:51] uh to preserve their own existence\n[01:36:54] already, right? They will lie to people.\n[01:36:57] They will blackmail them. They will they\n[01:37:00] will launch nuclear weapons rather than\n[01:37:03] uh be switched off. And so there's no\n[01:37:06] there's no positive sign that we're\n[01:37:08] getting any closer to safety with these\n[01:37:11] systems. In fact, the signs seem to be\n[01:37:12] that we're going uh deeper and deeper\n[01:37:15] into uh into dangerous behaviors. So\n[01:37:19] rather than say ban, I would just say\n[01:37:22] prove to us that the risk is less than\n[01:37:24] one in a 100 million per year of\n[01:37:26] extinction or loss of control, let's\n[01:37:28] say. And uh so we're not banning\n[01:37:32] anything.\n[01:37:34] The company's response is, \"Well, we\n[01:37:36] don't know how to do that, so you can't\n[01:37:38] have a rule.\"\n[01:37:41] Literally, they are saying, \"Humanity\n[01:37:44] has no right to protect itself from us.\"\n[01:37:48] >> If I was an alien looking down on planet\n[01:37:50] Earth right now, I would find this\n[01:37:51] fascinating\n[01:37:53] that these\n[01:37:54] >> Yeah. You're in the bar betting on\n[01:37:55] who's, [laughter] you know, are they\n[01:37:56] going to make it or not.\n[01:37:57] >> Just a really interesting experiment in\n[01:38:00] like human incentives. the analogy you\n[01:38:02] gave of there being this quadr\n[01:38:04] quadrillion dollar magnet pulling us off\n[01:38:06] the edge of the cliff\n[01:38:08] and yet we're still being drawn towards\n[01:38:12] it through greed and this promise of\n[01:38:13] abundance and power and status and I'm\n[01:38:15] going to be the one that summoned the\n[01:38:17] god\n[01:38:18] >> I mean it says something about us as\n[01:38:20] humans\n[01:38:21] says something about our our darker\n[01:38:24] sides\n[01:38:26] >> yes and the aliens will write an amazing\n[01:38:29] tragic play cycle\n[01:38:32] about what happened to the human race.\n[01:38:35] >> Maybe the AI is the alien\n[01:38:37] [clears throat] and it's going to talk\n[01:38:38] about, you know, we have our our stories\n[01:38:40] about God making the world in seven days\n[01:38:43] and Adam and Eve. Maybe it'll have its\n[01:38:45] own religious stories about\n[01:38:48] the God that made it us and how it\n[01:38:50] sacrificed itself. Just like Jesus\n[01:38:53] sacrificed himself for us, we sacrificed\n[01:38:55] ourselves for it.\n[01:38:58] >> Yeah. which is the wrong way around,\n[01:39:01] right? [laughter]\n[01:39:03] >> But that is that is the story of that's\n[01:39:04] that's the Judeo-Christian story, isn't\n[01:39:07] it? That God, you know, Jesus gave his\n[01:39:09] life for us so that we could be here\n[01:39:12] full of sin.\n[01:39:14] >> But is yeah, God is still watching over\n[01:39:16] us and uh probably wondering when we're\n[01:39:20] going to get our act together.\n[01:39:22] >> What is the most important thing we\n[01:39:24] haven't talked about that we should have\n[01:39:25] talked about, Professor Stuart Russell?\n[01:39:27] So I think um [clears throat]\n[01:39:30] the question of whether it's possible to\n[01:39:34] make\n[01:39:36] uh super intelligent AI systems that we\n[01:39:39] can control\n[01:39:40] >> is it possible?\n[01:39:41] >> I I think yes. I think it's possible and\n[01:39:43] I think we need to actually just have a\n[01:39:48] different conception of what it is we're\n[01:39:49] trying to build. For a long time with\n[01:39:53] with AI, we've just had this notion of\n[01:39:56] pure intelligence, right? The the\n[01:39:59] ability to bring about whatever future\n[01:40:02] you, the intelligent entity, want to\n[01:40:05] bring about.\n[01:40:05] >> The more intelligence, the better.\n[01:40:06] >> The more intelligent the better and the\n[01:40:08] more capability it will have to create\n[01:40:11] the future that it wants. And actually\n[01:40:13] we don't want pure intelligence\n[01:40:18] because\n[01:40:20] what the future that it wants might not\n[01:40:22] be the future that we want. There's\n[01:40:25] nothing particle\n[01:40:28] humans out as the the only thing that\n[01:40:30] matters,\n[01:40:32] right? You know, pure intelligence might\n[01:40:34] decide that actually it's going to make\n[01:40:36] life wonderful for cockroaches or or\n[01:40:39] actually doesn't care about biological\n[01:40:41] life at all.\n[01:40:43] We actually want intelligence whose only\n[01:40:47] purpose is to bring about the future\n[01:40:50] that we want. Right? So it's we want it\n[01:40:53] to be first of all keyed to humans\n[01:40:57] specifically not to cockroaches not to\n[01:40:59] aliens not to itself.\n[01:41:00] >> We want to make it loyal to humans.\n[01:41:02] >> Right? So keyed to humans\n[01:41:05] and the difficulty that I mentioned\n[01:41:06] earlier right the king Midas problem.\n[01:41:09] How do we specify\n[01:41:11] what we want the future to be like so\n[01:41:13] that it can do it for us? How do we\n[01:41:15] specify the objectives?\n[01:41:17] Actually, we have to give up on that\n[01:41:19] idea because it's not possible. Right?\n[01:41:22] We've seen this over and over again in\n[01:41:24] human history. Uh we don't know how to\n[01:41:26] specify the future properly. We don't\n[01:41:29] know how to say what we want. And uh you\n[01:41:32] know, I always use the example of the\n[01:41:34] genie, right? What's the third wish that\n[01:41:37] you give to the genie who's granted you\n[01:41:39] three wishes? Right? Undo the first two\n[01:41:42] wishes because I made a mess of the\n[01:41:43] universe.\n[01:41:46] >> So, um, so in fact, what we're going to\n[01:41:49] do is\n[01:41:51] we're going to make it the machine's job\n[01:41:54] to figure out. So, it has to bring about\n[01:41:56] the future that we want,\n[01:41:59] but\n[01:42:02] it has to figure out what that is. And\n[01:42:05] it's going to start out not knowing.\n[01:42:09] And uh\n[01:42:11] over time through interacting with us\n[01:42:13] and observing the choices we make, it\n[01:42:16] will learn more about what we want the\n[01:42:18] future to be like.\n[01:42:20] But probably it will forever have\n[01:42:25] residual uncertainty\n[01:42:27] about what we really want the future to\n[01:42:30] be like. It'll it'll be fairly sure\n[01:42:32] about some things and it can help us\n[01:42:33] with those.\n[01:42:34] and it'll be uncertain about other\n[01:42:36] things and it'll be uh in those cases it\n[01:42:39] will not take action that might upset\n[01:42:45] humans with that you know with that\n[01:42:46] aspect of the world. So to give you a\n[01:42:48] simple example right um what color do we\n[01:42:51] want the sky to be?\n[01:42:54] It's not sure. So it shouldn't mess with\n[01:42:56] the sky\n[01:42:58] unless it knows for sure that we really\n[01:43:00] want purple with green stripes.\n[01:43:02] Everything you're saying sounds like\n[01:43:04] we're creating\n[01:43:06] a god. Like earlier on I was saying that\n[01:43:08] we are the god but actually everything\n[01:43:10] you described there almost sounds like\n[01:43:12] every every god in religion where you\n[01:43:14] know we pray to gods but they don't\n[01:43:16] always do anything about it.\n[01:43:17] >> Not not exactly. No it's it's in some\n[01:43:20] sense I'm thinking more like the ideal\n[01:43:23] butler. To the extent that the butler\n[01:43:25] can anticipate your wishes they should\n[01:43:28] help you bring them about. But in in\n[01:43:31] areas where there's uncertainty, it can\n[01:43:33] ask questions. We can we can make\n[01:43:36] requests.\n[01:43:37] >> This sounds like God to me because, you\n[01:43:38] know, I might say to God or this butler,\n[01:43:42] uh, could you go get me my uh my car\n[01:43:44] keys from upstairs? And its assessment\n[01:43:46] would be, listen, if I do this for this\n[01:43:48] person, then their muscles are going to\n[01:43:50] atrophy. Then they're going to lose\n[01:43:51] meaning in their life. Then they're not\n[01:43:53] going to know how to do hard things. So\n[01:43:54] I won't get involved. It's an\n[01:43:56] intelligence that sits in. But actually,\n[01:43:57] probably in most situations, it\n[01:44:00] optimizing for comfort for me or doing\n[01:44:01] things for me is actually probably not\n[01:44:02] in my best long-term interests. It's\n[01:44:04] probably it's probably useful that I\n[01:44:06] have a girlfriend and argue with her and\n[01:44:08] that I like raise kids and that I walk\n[01:44:10] to the shop and get my own stuff.\n[01:44:12] >> I agree with you. I mean, I think that's\n[01:44:14] So, you're putting your finger on\n[01:44:16] uh in some sense sort of version 2.0,\n[01:44:20] right? So, let's get version 1.0 clear,\n[01:44:23] right? this this form of AI where\n[01:44:28] it has to further our interest but it\n[01:44:30] doesn't know what those interests are\n[01:44:32] right it then puts an obligation on it\n[01:44:34] to learn more and uh to be helpful where\n[01:44:37] it understands well enough and to be\n[01:44:39] cautious where it doesn't understand\n[01:44:41] well so on so that that actually we can\n[01:44:45] formulate as a mathematical problem and\n[01:44:47] at least under idealized circumstances\n[01:44:50] we can literally solve that So we can\n[01:44:53] make AI systems that know how to solve\n[01:44:57] this problem and help the entities that\n[01:44:59] they are interacting with.\n[01:45:00] >> The reason I make the God analogy is\n[01:45:02] because I think that such a being, such\n[01:45:04] an intelligence would realize the\n[01:45:06] importance of equilibrium in the world.\n[01:45:08] Pain and pleasure, good and evil, and\n[01:45:11] then it would\n[01:45:12] >> absolutely\n[01:45:13] >> and then it would be like this.\n[01:45:14] >> So So right. So yes, I mean that's sort\n[01:45:18] of what happens in the matrix, right?\n[01:45:19] They tried the the AI systems in the\n[01:45:21] matrix, they tried to give us a utopia,\n[01:45:25] but it failed miserably and uh you know,\n[01:45:28] fields and fields of humans had to be\n[01:45:30] destroyed. Um, and the best they could\n[01:45:33] come up with was, you know, late 20th\n[01:45:34] century regular human life with all of\n[01:45:37] its problems, right? And I think this is\n[01:45:40] a really interesting point\n[01:45:43] and absolutely central because you know\n[01:45:45] there's a lot of science fiction where\n[01:45:48] super intelligent robots you know they\n[01:45:51] just want to help humans and the humans\n[01:45:54] who don't like that you know they just\n[01:45:56] give them a little brain operation to\n[01:45:58] then they do like it. Um and it takes\n[01:46:01] away human motivation.\n[01:46:05] uh it it by taking away failure uh\n[01:46:09] taking away disease you actually lose\n[01:46:12] important parts of human life and it\n[01:46:14] becomes in some sense pointless. So if\n[01:46:17] it turns out\n[01:46:19] that there simply isn't any way that\n[01:46:23] humans can really flourish\n[01:46:27] in coexistence with super intelligent\n[01:46:29] machines, even if they're perfectly\n[01:46:32] designed to to to solve this problem of\n[01:46:35] figuring out what humans what futures uh\n[01:46:38] humans want and and bringing about those\n[01:46:40] futures.\n[01:46:43] If that's not possible, then those\n[01:46:45] machines will actually disappear.\n[01:46:49] >> Why would they disappear?\n[01:46:50] >> Because that's the best thing for us.\n[01:46:53] Maybe they would stay available for real\n[01:46:57] existential emergencies, like if there\n[01:46:59] is a giant asteroid about to hit the\n[01:47:00] earth that maybe they'll help us uh\n[01:47:02] because they at least want the human\n[01:47:04] species to continue. But to some extent,\n[01:47:07] it's not a perfect analogy, but it's\n[01:47:09] it's sort of the way that human parents\n[01:47:12] have to at some point step back from\n[01:47:15] their kids' lives and say, \"Okay, no,\n[01:47:17] you have to tie your own shoelaces\n[01:47:19] today.\"\n[01:47:20] >> This is kind of what I was thinking.\n[01:47:21] Maybe there was uh a civilization before\n[01:47:24] us and they arrived at this moment in\n[01:47:26] time where they created an intelligence\n[01:47:31] and that intelligence did all the things\n[01:47:33] you've said and it realized the\n[01:47:35] importance of equilibrium. So it decided\n[01:47:36] not to get involved and\n[01:47:40] maybe at some level\n[01:47:43] that's the god we look up to the stars\n[01:47:45] and worship one that's not really\n[01:47:47] getting involved and letting things play\n[01:47:48] out however however they are. but might\n[01:47:50] step in in the case of a real\n[01:47:52] existential emergency.\n[01:47:53] >> Maybe, maybe not. Maybe. [laughter] But\n[01:47:55] then and then maybe the cycle repeats\n[01:47:57] itself where you know the organisms it\n[01:47:59] let have free will end up creating the\n[01:48:02] same intelligence and then the universe\n[01:48:06] perpetuates infinitely.\n[01:48:08] >> Yep. There there are science fiction\n[01:48:10] stories like that too. [laughter] Yeah.\n[01:48:12] I hope there is some happy medium where\n[01:48:17] the AI systems can be there and we can\n[01:48:20] take advantage of of those capabilities\n[01:48:23] to have a civilization that's much\n[01:48:25] better than the one we have now.\n[01:48:28] Um, but I think you're right. A\n[01:48:30] civilization with no challenges\n[01:48:33] is not uh is not conducive to human\n[01:48:37] flourishing.\n[01:48:37] >> What can the average person do, Stuart?\n[01:48:40] average person listening to this now to\n[01:48:42] aid the cause that you're fighting for.\n[01:48:45] >> I actually think um you know this sounds\n[01:48:47] corny but you know talk to your\n[01:48:49] representative, your MP, your\n[01:48:51] congressperson, whatever it is. Um\n[01:48:54] because\n[01:48:56] I think the policy makers need to hear\n[01:48:58] from people. The only voices they're\n[01:49:00] hearing right now are the tech companies\n[01:49:04] and their $50 billion checks.\n[01:49:08] And um\n[01:49:10] all the polls that have been done say\n[01:49:13] yeah most people 80% maybe don't want\n[01:49:17] there to be super intelligent machines\n[01:49:20] but they don't know what to do. You know\n[01:49:22] even for me I've been in this field for\n[01:49:25] decades.\n[01:49:26] uh I'm not sure what to do because of\n[01:49:30] this giant magnet pulling everyone\n[01:49:32] forward and uh and the vast sums of\n[01:49:35] money being being put into this. Um, but\n[01:49:38] I am sure that if you want to have a\n[01:49:41] future\n[01:49:43] and a world that you want your kids to\n[01:49:45] live in, uh, you need to make your voice\n[01:49:49] heard\n[01:49:52] and, uh, and I think governments will\n[01:49:54] listen\n[01:49:55] from a political point of view, right?\n[01:49:58] You put your finger in the wind and you\n[01:50:02] say, \"hm, should I be on the side of\n[01:50:04] humanity or our future robot overlords?\"\n[01:50:09] I think I think as a politician, it's\n[01:50:11] not a difficult decision.\n[01:50:14] >> It is when you've got someone saying,\n[01:50:15] \"I'll give you $50 billion.\"\n[01:50:18] >> Exactly. So, um I think I think people\n[01:50:22] in those positions of power need to hear\n[01:50:25] from their constituents\n[01:50:28] um that this is not the direction we\n[01:50:30] want to go.\n[01:50:30] >> After committing your career to this\n[01:50:33] subject and the subject of technology\n[01:50:34] more broadly, but specifically being the\n[01:50:36] guy that wrote the book about artificial\n[01:50:38] intelligence,\n[01:50:42] you must realize that you're living in a\n[01:50:44] historical moment. Like there's very few\n[01:50:46] times in my life where I go, \"Oh, this\n[01:50:47] is one of those moments. This is a\n[01:50:50] crossroads in history.\" And it must to\n[01:50:52] some degree weigh upon you knowing that\n[01:50:55] you're a person of influence at this\n[01:50:56] historical moment in time who could\n[01:50:58] theoretically\n[01:51:00] help divert the course of history in\n[01:51:03] this moment in time. It's kind of like\n[01:51:04] the you look through history, you see\n[01:51:05] these moments of like Oenheimer and um\n[01:51:08] does it weigh on you when you're alone\n[01:51:10] at night thinking to yourself and\n[01:51:12] reading things?\n[01:51:13] >> Yeah, it does. I mean, you know, after\n[01:51:14] 50 years, I could retire and um, you\n[01:51:17] know, play golf and sing and sail and do\n[01:51:19] things that I enjoy. Um,\n[01:51:23] but instead, I'm working 80 or 100 hours\n[01:51:25] a week\n[01:51:27] um trying to move\n[01:51:29] uh move things in the right direction.\n[01:51:31] >> What is that narrative in your head\n[01:51:33] that's making you do that? Like what is\n[01:51:34] the is there an element of I might\n[01:51:37] regret this if I don't or\n[01:51:39] >> just it's it's not only the the right\n[01:51:43] thing to do it's it's completely\n[01:51:45] essential. I mean there isn't\n[01:51:50] there isn't a bigger motivation\n[01:51:54] than this.\n[01:51:56] >> Do you feel like you're winning or\n[01:51:58] losing?\n[01:51:59] It feels um\n[01:52:03] like things are moving somewhat in the\n[01:52:06] right direction. You know, it's a a\n[01:52:07] ding-dong battle as uh as David Coleman\n[01:52:12] used to say in uh in the exciting\n[01:52:14] football match in 2023, right? So, uh\n[01:52:18] GPT4 came out and then we issued the\n[01:52:21] pause statement that was signed by a lot\n[01:52:24] of leading AI researchers. Um and then\n[01:52:28] in May there was the extinction\n[01:52:29] statement which included\n[01:52:32] uh Sam Holman and Deis Sabis and Dario\n[01:52:35] Amade other CEOs as well saying yeah\n[01:52:37] this is an extinction risk on the level\n[01:52:39] with nuclear war and I think governments\n[01:52:43] listened at that point the UK government\n[01:52:47] earlier that year had said oh well you\n[01:52:48] know we don't need to regulate AI you\n[01:52:50] know full speed ahead technology is good\n[01:52:52] for you and by June they had completely\n[01:52:57] changed and Rishi Sununnak announced\n[01:53:00] that he was going to hold this global AI\n[01:53:02] safety summit uh in England and he\n[01:53:05] wanted London to be the global hub for\n[01:53:08] AI regulation\n[01:53:10] um and so on. So and then you know when\n[01:53:15] beginning of November of 23 28 countries\n[01:53:18] including the US and China signed a\n[01:53:20] declaration\n[01:53:22] saying you know AI presents catastrophic\n[01:53:25] risks and it's urgent that we address\n[01:53:26] them and so on. So there it felt like,\n[01:53:29] wow, they're listening. They're going to\n[01:53:33] do something about it.\n[01:53:35] And then I think, you know, the am the\n[01:53:37] amount of money going into AI was\n[01:53:39] already ramping up\n[01:53:42] and the tech companies pushed back\n[01:53:46] and this narrative took hold that um the\n[01:53:50] US in particular has to win the race\n[01:53:52] against China.\n[01:53:54] The Trump administration completely\n[01:53:56] dismissed\n[01:53:58] uh any concerns about safety explicitly.\n[01:54:01] And interestingly, right, I mean they\n[01:54:02] did that as far as I can tell directly\n[01:54:05] in response to the accelerationists such\n[01:54:09] as Mark Andre going to Washington or\n[01:54:12] sorry going to Trump before the election\n[01:54:16] and saying if I give you X amount of\n[01:54:18] money will you announce that there will\n[01:54:22] be no regulation of AI and Trump said\n[01:54:25] yes you know probably like what is AI\n[01:54:28] doesn't matter as long as we give you\n[01:54:29] the money right okay uh Uh so they gave\n[01:54:33] him the money and he said there's going\n[01:54:34] to be no regulation of AI. Up to that\n[01:54:36] point it was a bipartisan\n[01:54:39] issue in Washington. Both parties were\n[01:54:42] concerned. Both parties were on the side\n[01:54:44] of the human race against the robot\n[01:54:46] overlords.\n[01:54:47] Uh and that moment turned it into a\n[01:54:50] partisan issue. The\n[01:54:53] after the election the US put pressure\n[01:54:56] on the French who are the next hosts of\n[01:54:58] the global AI summit.\n[01:55:01] uh and that was in February of this year\n[01:55:04] and uh and that summit turned in from\n[01:55:07] you know what had been focused largely\n[01:55:10] on safety in the UK to a summit that\n[01:55:13] looked more like a trade show. So it was\n[01:55:15] focused largely on money and so that was\n[01:55:18] sort of the Nadia right you know the\n[01:55:19] pendulum swung because of corporate\n[01:55:22] pressure uh and their ability to take\n[01:55:25] over the the political dimension.\n[01:55:28] Um, but I would say since then things\n[01:55:31] have been moving back again. So I'm\n[01:55:33] feeling a bit more optimistic than I did\n[01:55:35] in February. You know, we have a a\n[01:55:39] global movement now. There's an\n[01:55:40] international association for safe and\n[01:55:42] ethical AI\n[01:55:44] uh which has several thousand members\n[01:55:46] and um more than 120 organizations in\n[01:55:52] dozens of countries are affiliates of\n[01:55:55] this global organization.\n[01:55:57] Um, so I'm\n[01:56:00] I'm thinking that if we can in\n[01:56:02] particular if we can activate public\n[01:56:03] opinion\n[01:56:05] which which works through the media and\n[01:56:08] through popular culture uh then we have\n[01:56:11] a chance\n[01:56:13] >> seen such a huge appetite to learn about\n[01:56:15] these subjects from our audience.\n[01:56:18] We know when Jeffrey Hinton came on the\n[01:56:19] show I think about 20 million people\n[01:56:21] downloaded or streamed that conversation\n[01:56:23] which was staggering. and the the other\n[01:56:26] conversations we've had about AI safety\n[01:56:27] with othera safety experts have done\n[01:56:30] exactly the same it says something it\n[01:56:33] kind of reflects what you were saying\n[01:56:34] about the 80% of the population are\n[01:56:36] really concerned and don't want this but\n[01:56:38] that's not what you see in the sort of\n[01:56:39] commercial world and listen I um I have\n[01:56:41] to always acknowledge my own my own\n[01:56:44] apparent contradiction because I am both\n[01:56:46] an investor in companies that are\n[01:56:48] accelerating AI but at the same time\n[01:56:50] someone who spends a lot of time on my\n[01:56:52] podcast speaking to people that are\n[01:56:53] warning against the risk And actually\n[01:56:55] like there's many ways you can look at\n[01:56:56] this. I used to work in social media for\n[01:56:57] for six or seven years built one of the\n[01:56:59] big social media marketing companies in\n[01:57:01] Europe and people would often ask me is\n[01:57:03] like social media a good thing or a bad\n[01:57:04] thing and I'd talk about the bad parts\n[01:57:05] of it and then they'd say you know\n[01:57:07] you're building a social media company\n[01:57:09] you're not contributing to the problem.\n[01:57:11] Well I think I think that like binary\n[01:57:13] way of thinking is often the problem. It\n[01:57:15] the binary way of thinking that like\n[01:57:17] it's all bad or it's all really really\n[01:57:18] good is like often the problem and that\n[01:57:19] this push to put you into a camp.\n[01:57:21] Whereas I think the most uh\n[01:57:23] intellectually honest and high integrity\n[01:57:25] people I know can point at both the bad\n[01:57:27] and the good.\n[01:57:27] >> Yeah. I I think it's it's bizarre to be\n[01:57:31] accused of being anti- AI uh to be\n[01:57:35] called a lite. Um you know as I said\n[01:57:38] when I wrote the book on which from\n[01:57:40] which almost everyone learns about AI um\n[01:57:44] and uh you know is it if you called a\n[01:57:49] nuclear engineer who works on the safety\n[01:57:51] of nuclear power plants would you call\n[01:57:53] him anti-ysics\n[01:57:56] right it's it's bizarre right it's we're\n[01:57:58] not anti- AAI in fact\n[01:58:02] the need for safety in AI is a\n[01:58:04] complement to AI right if AI was useless\n[01:58:07] and stupid, we wouldn't be worried about\n[01:58:09] uh its safety. It's only because it's\n[01:58:12] becoming more capable that we have to be\n[01:58:14] concerned about safety.\n[01:58:16] Uh so I don't see this as anti-AI at\n[01:58:19] all. In fact, I would say without\n[01:58:21] safety, there will be no AI,\n[01:58:24] right? There is no future with human\n[01:58:27] beings where we have unsafe AI. So it's\n[01:58:31] either no AI or safe AI.\n[01:58:34] We have a closing tradition on this\n[01:58:36] podcast where the last guest leaves a\n[01:58:37] question for the next, not knowing who\n[01:58:38] they're leaving it for. And the question\n[01:58:40] left for you is, what do you value the\n[01:58:42] most in life and why? And lastly, how\n[01:58:47] many times has this answer changed?\n[01:58:51] >> Um,\n[01:58:54] I value my family most and that answer\n[01:58:57] hasn't changed for nearly 30 years.\n[01:59:01] What else outside of your family?\n[01:59:03] >> Truth.\n[01:59:07] And that Yeah, that answer hasn't\n[01:59:09] changed at all. I I've always\n[01:59:14] wanted the world to base its life on\n[01:59:17] truth.\n[01:59:18] And I find the propagation or deliberate\n[01:59:22] propagation of falsehood uh to be one of\n[01:59:25] the worst things that we can do. even if\n[01:59:28] that truth is inconvenient.\n[01:59:30] >> Yeah,\n[01:59:32] >> I think that's a really important point\n[01:59:34] which is that you know people people\n[01:59:36] often don't like hearing things that are\n[01:59:38] negative and so the visceral reaction is\n[01:59:40] often to just shoot or aim at the person\n[01:59:42] who is delivering the bad news because\n[01:59:44] if I discredit you or I shoot at you\n[01:59:47] then it makes it easier for me to\n[01:59:49] contend with the news that I don't like,\n[01:59:51] the thing that's making me feel\n[01:59:52] uncomfortable. And so I I applaud you\n[01:59:54] for what you're doing because you're\n[01:59:56] going to get lots of shots taken at you\n[01:59:58] because you're delivering an\n[01:59:59] inconvenient truth which generally\n[02:00:00] people won't won't always love. But also\n[02:00:03] you are messing with people's ability to\n[02:00:05] get that quadrillion dollar prize which\n[02:00:08] means there'll be more deliberate\n[02:00:09] attempts to discredit people like\n[02:00:10] yourself and Jeff Hinton and other\n[02:00:12] people that I've spoken to on the show.\n[02:00:13] But again, when I look back through\n[02:00:14] history, I think that progress has come\n[02:00:16] from the pursuit of truth even when it\n[02:00:17] was inconvenient. And actually much of\n[02:00:19] the luxuries that I value in my life are\n[02:00:21] the consequence of other people that\n[02:00:23] came before me that were brave enough or\n[02:00:24] bold enough to pursue truth at times\n[02:00:27] when it was inconvenient.\n[02:00:29] >> And so I very much respect and value\n[02:00:31] people like yourself for that very\n[02:00:32] reason. You've written this incredible\n[02:00:33] book called human compatible artificial\n[02:00:35] intelligence and the problem of control\n[02:00:37] which I think was published in 2020.\n[02:00:39] >> 2019. Yeah. There's a new edition from\n[02:00:41] 2023.\n[02:00:43] >> Where do people go if they want more\n[02:00:44] information on your work and you do they\n[02:00:46] go to your website? Do they get this\n[02:00:48] book? what's the best place for them to\n[02:00:49] learn more?\n[02:00:49] >> So, so the book is written for the\n[02:00:51] general public. Um, I'm easy to find on\n[02:00:54] the web. The information on my web page\n[02:00:56] is mostly targeted for academics. So,\n[02:00:58] it's a lot of technical research papers\n[02:01:01] and so on. Um, there is an organization\n[02:01:04] as I mentioned called the International\n[02:01:06] Association for Safe and Ethical AI. Uh,\n[02:01:09] that has a a website. It has a terrible\n[02:01:11] acronym unfortunately, I AI. We\n[02:01:15] pronounce it ICI but it uh it's easy to\n[02:01:17] misspell but you can find that on the\n[02:01:19] web as well and that has uh that has\n[02:01:21] resources uh you can join the\n[02:01:23] association\n[02:01:25] uh you can apply to come to our annual\n[02:01:28] conference and you know I think\n[02:01:29] increasingly not you know not just AI\n[02:01:33] researchers like Jeff Hinton Yosha\n[02:01:35] Benjio but also I think uh you know\n[02:01:39] writers Brian Christian for example has\n[02:01:41] a nice book called the alignment problem\n[02:01:44] Um\n[02:01:46] and uh he's looking at it from the\n[02:01:48] outside. He's not\n[02:01:50] or at least when he wrote it, he wasn't\n[02:01:52] an AI researcher. He's now becoming one.\n[02:01:54] Um\n[02:01:56] but uh he he has talked to many of the\n[02:01:59] people involved in these questions uh\n[02:02:01] and tries to give an objective view. So\n[02:02:03] I think it's a it's a pretty good book.\n[02:02:06] >> I will link all of that below for anyone\n[02:02:07] that wants to check out any of those\n[02:02:09] links and learn more.\n[02:02:11] Professor Stuart Russell, thank you so\n[02:02:12] much. really appreciate you taking the\n[02:02:14] time and the effort to come and have\n[02:02:15] this conversation and I think uh I think\n[02:02:17] it's pushing the public conversation in\n[02:02:19] a in an important direction.\n[02:02:21] >> Thanks you\n[02:02:22] >> and I applaud you for doing that.\n[02:02:23] >> Really nice talking to you. [music]\n[02:02:28] >> I'm absolutely obsessed with 1%. If you\n[02:02:30] know me, if you follow Behind the Diary,\n[02:02:31] which is our behind the scenes channel,\n[02:02:32] if you've heard me speak on stage, if\n[02:02:34] you follow me on any social media\n[02:02:35] channel, you've probably heard me\n[02:02:36] talking about 1%. It is the defining\n[02:02:38] philosophy of my health, of my\n[02:02:40] companies, of my habit formation and\n[02:02:43] everything in between, which is this\n[02:02:44] obsessive focus on the small things.\n[02:02:46] Because sometimes in life, we aim at\n[02:02:48] really, really, really, really big\n[02:02:49] things, big steps forward. Mountains we\n[02:02:51] have to climb. And as NAL told me on\n[02:02:54] this podcast, when you aim at big\n[02:02:55] things, you get psychologically\n[02:02:57] demotivated. You end up procrastinating,\n[02:02:59] avoiding them, and change never happens.\n[02:03:01] So, with that in mind, with everything\n[02:03:02] I've learned about 1% and with\n[02:03:04] everything I've learned from\n[02:03:04] interviewing the incredible guests on\n[02:03:05] this podcast, we made the 1% diary just\n[02:03:08] over a year ago and it sold out. And it\n[02:03:11] is the best feedback we've ever had on a\n[02:03:13] diary that we have created because what\n[02:03:15] it does is it takes you through this\n[02:03:17] incredible process over 90 days to help\n[02:03:19] you build and form brand new habits. So,\n[02:03:23] if you want to get one for yourself or\n[02:03:24] you want to get one for your team, your\n[02:03:26] company, a friend, a sibling, anybody\n[02:03:28] that listens to the diary of a co, head\n[02:03:30] over immediately to the diary.com\n[02:03:34] and you can inquire there about getting\n[02:03:35] a bundle if you want to get one for your\n[02:03:36] team or for a large group of people.\n[02:03:38] That is the diary.com.\n[02:03:42] [music]\n[02:03:48] [music]\n[02:03:52] Heat. Heat.\n[02:03:58] [singing]\n[02:04:00] >> [music]",
  "transcript_source": "youtube_captions",
  "has_timestamps": true
}