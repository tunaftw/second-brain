# Podcast Nuggets - Master Plan

> **Syfte:** Detta dokument √§r en komplett blueprint f√∂r att bygga `podcast-nuggets` - ett personligt kunskapsbibliotek. Kopiera denna fil till ditt `podcast-nuggets`-repo och k√∂r `/brainstorm` i Claude Code f√∂r att l√§sa planen och p√•b√∂rja implementation.

---

## Projekt√∂versikt

**podcast-nuggets** √§r ett personligt kunskapsbibliotek som f√•ngar insikter fr√•n:
- **YouTube** (prim√§r k√§lla f√∂r transkript) - Huberman Lab, Joe Rogan, Chris Williamson, Diary of a CEO, m.fl.
- **Podcasts** (n√§r inneh√•ll inte finns p√• YouTube)
- **Twitter/X** - tweets, tr√•dar, och artiklar l√§nkade fr√•n Twitter

### Vad systemet producerar

1. **Taggade takeaways** - Korta insikter med kategorier och taggar, s√∂kbara
2. **Strukturerade anteckningar** - L√§ngre sammanfattningar per k√§lla
3. **Citat** - Minnesv√§rda quotes med kontext och talare
4. **Kunskapskort** - Flashcard-format: koncept + f√∂rklaring
5. **Action items** - Konkreta saker att g√∂ra

### Analysmetoder (anv√§ndaren v√§ljer)

| Metod | Beskrivning | N√§r |
|-------|-------------|-----|
| **Claude Code** | Direkt i konversation, interaktivt | Enstaka analyser, snabb feedback |
| **OpenCode/GLM-4.7** | Python-script i separat terminal, gratis | Batch-k√∂rning, stora volymer |

### Lagring

- **JSON-filer** som source of truth (l√§sbara, versionerbara)
- **SQLite** som s√∂kbart index (synkas fr√•n JSON)
- **Framtida:** Vector embeddings f√∂r semantisk s√∂kning, Apple Notes export

---

## Mappstruktur

```
podcast-nuggets/
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îî‚îÄ‚îÄ skills/                        # Claude Code skills
‚îÇ       ‚îú‚îÄ‚îÄ youtube-download/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ SKILL.md
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ references/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ download-method.md
‚îÇ       ‚îú‚îÄ‚îÄ podcast-download/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ SKILL.md
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ references/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ download-method.md
‚îÇ       ‚îú‚îÄ‚îÄ twitter-download/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ SKILL.md
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ references/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ download-method.md
‚îÇ       ‚îú‚îÄ‚îÄ analyze/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ SKILL.md
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ references/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ claude-method.md
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ opencode-method.md
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ output-schema.md
‚îÇ       ‚îú‚îÄ‚îÄ database-sync/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ SKILL.md
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ references/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ sync-method.md
‚îÇ       ‚îî‚îÄ‚îÄ search/
‚îÇ           ‚îú‚îÄ‚îÄ SKILL.md
‚îÇ           ‚îî‚îÄ‚îÄ references/
‚îÇ               ‚îî‚îÄ‚îÄ search-method.md
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ nuggets/                       # Python-moduler
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ analyze/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ prompt_builder.py      # Bygger analys-prompts
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ validator.py           # Pydantic-validering
‚îÇ       ‚îú‚îÄ‚îÄ db/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ models.py              # SQLAlchemy-modeller
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ sync.py                # JSON ‚Üí SQLite sync
‚îÇ       ‚îî‚îÄ‚îÄ export/
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îî‚îÄ‚îÄ apple_notes.py         # Framtida Apple Notes export
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ analyze_batch.py               # OpenCode batch-analys
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ youtube/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcripts/               # R√•a transkript
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyses/                  # JSON-analyser
‚îÇ   ‚îú‚îÄ‚îÄ podcasts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyses/
‚îÇ   ‚îú‚îÄ‚îÄ twitter/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources/                   # Tweets, artiklar
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyses/
‚îÇ   ‚îú‚îÄ‚îÄ nuggets.db                     # SQLite s√∂kindex
‚îÇ   ‚îî‚îÄ‚îÄ categories.json                # Konfigurerbara kategorier
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md
```

---

## Kategorier

Flexibla kategorier som kan ut√∂kas √∂ver tid.

**`data/categories.json`:**
```json
{
  "categories": [
    {"id": "philosophy", "name": "Filosofi & Livsvisdom", "emoji": "üß†"},
    {"id": "career", "name": "Karri√§r & Professionell utveckling", "emoji": "üíº"},
    {"id": "health", "name": "H√§lsa & Tr√§ning", "emoji": "üí™"},
    {"id": "relationships", "name": "Relationer & Kommunikation", "emoji": "ü§ù"},
    {"id": "productivity", "name": "Produktivitet & Vanor", "emoji": "‚ö°"},
    {"id": "creativity", "name": "Kreativitet & L√§rande", "emoji": "üé®"}
  ]
}
```

---

## JSON-schema f√∂r analyser

Varje analyserad k√§lla producerar en JSON-fil med detta schema:

**`data/{source}/analyses/{id}.json`:**
```json
{
  "source": {
    "type": "youtube|podcast|twitter",
    "url": "https://...",
    "title": "Andrew Huberman: Sleep Optimization",
    "author": "Huberman Lab",
    "date": "2025-01-15",
    "duration_minutes": 120
  },
  "summary": "Kortfattad sammanfattning av hela inneh√•llet. 2-3 meningar som f√•ngar k√§rnan.",
  "takeaways": [
    {
      "id": "uuid-here",
      "text": "Morgonsol inom 30 min efter uppvaknande f√∂rb√§ttrar dygnsrytm",
      "category": "health",
      "tags": ["sleep", "circadian", "sunlight"],
      "confidence": "high|medium|low",
      "timestamp": "00:15:32"
    }
  ],
  "quotes": [
    {
      "id": "uuid-here",
      "text": "Your nervous system is not designed for chronic stress",
      "speaker": "Andrew Huberman",
      "context": "Diskussion om stress-respons och dess p√•verkan p√• s√∂mn",
      "timestamp": "00:42:18"
    }
  ],
  "knowledge_cards": [
    {
      "id": "uuid-here",
      "concept": "Adenosinuppbyggnad",
      "explanation": "Adenosin √§r en molekyl som byggs upp under vakenhet och skapar s√∂mnighet. Koffein blockerar adenosinreceptorer tillf√§lligt men tar inte bort adenosinet - d√§rf√∂r k√§nner man sig tr√∂tt n√§r koffeinet sl√§pper.",
      "category": "health",
      "related_concepts": ["caffeine", "sleep-pressure", "circadian-rhythm"]
    }
  ],
  "action_items": [
    "G√• ut i morgonsol inom 30 min efter uppvaknande",
    "Undvik koffein de f√∂rsta 90 minuterna efter uppvaknande",
    "H√•ll sovrummet m√∂rkt och svalt"
  ],
  "metadata": {
    "analyzed_at": "2025-01-20T14:30:00Z",
    "analysis_method": "claude-code|opencode",
    "model": "claude-opus-4-5-20251101|glm-4.7",
    "schema_version": "1.0",
    "embedding": null
  }
}
```

---

## SQLite-databasschema

**`data/nuggets.db`:**

```sql
-- K√§llor (YouTube, podcasts, Twitter)
CREATE TABLE sources (
    id TEXT PRIMARY KEY,
    type TEXT NOT NULL CHECK (type IN ('youtube', 'podcast', 'twitter')),
    url TEXT UNIQUE,
    title TEXT,
    author TEXT,
    date DATE,
    duration_minutes INTEGER,
    transcript_path TEXT,
    analysis_path TEXT,
    analyzed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Takeaways (s√∂kbara insikter)
CREATE TABLE takeaways (
    id TEXT PRIMARY KEY,
    source_id TEXT REFERENCES sources(id) ON DELETE CASCADE,
    text TEXT NOT NULL,
    category TEXT,
    tags TEXT,  -- JSON array: ["sleep", "circadian"]
    confidence TEXT CHECK (confidence IN ('high', 'medium', 'low')),
    timestamp TEXT,  -- "00:15:32"
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Citat
CREATE TABLE quotes (
    id TEXT PRIMARY KEY,
    source_id TEXT REFERENCES sources(id) ON DELETE CASCADE,
    text TEXT NOT NULL,
    speaker TEXT,
    context TEXT,
    timestamp TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Kunskapskort
CREATE TABLE knowledge_cards (
    id TEXT PRIMARY KEY,
    source_id TEXT REFERENCES sources(id) ON DELETE CASCADE,
    concept TEXT NOT NULL,
    explanation TEXT NOT NULL,
    category TEXT,
    related_concepts TEXT,  -- JSON array
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Action items
CREATE TABLE action_items (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_id TEXT REFERENCES sources(id) ON DELETE CASCADE,
    text TEXT NOT NULL,
    completed BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Kategorier (konfigurerbara)
CREATE TABLE categories (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    emoji TEXT,
    sort_order INTEGER DEFAULT 0
);

-- Index f√∂r snabbare queries
CREATE INDEX idx_takeaways_category ON takeaways(category);
CREATE INDEX idx_takeaways_source ON takeaways(source_id);
CREATE INDEX idx_quotes_speaker ON quotes(speaker);
CREATE INDEX idx_knowledge_cards_category ON knowledge_cards(category);
CREATE INDEX idx_sources_type ON sources(type);
CREATE INDEX idx_sources_author ON sources(author);

-- FTS5 f√∂r fulltext-s√∂kning
CREATE VIRTUAL TABLE takeaways_fts USING fts5(
    text,
    tags,
    content=takeaways,
    content_rowid=rowid
);

CREATE VIRTUAL TABLE quotes_fts USING fts5(
    text,
    speaker,
    context,
    content=quotes,
    content_rowid=rowid
);

CREATE VIRTUAL TABLE knowledge_cards_fts USING fts5(
    concept,
    explanation,
    content=knowledge_cards,
    content_rowid=rowid
);

-- Triggers f√∂r att h√•lla FTS synkad
CREATE TRIGGER takeaways_ai AFTER INSERT ON takeaways BEGIN
    INSERT INTO takeaways_fts(rowid, text, tags) VALUES (NEW.rowid, NEW.text, NEW.tags);
END;

CREATE TRIGGER quotes_ai AFTER INSERT ON quotes BEGIN
    INSERT INTO quotes_fts(rowid, text, speaker, context) VALUES (NEW.rowid, NEW.text, NEW.speaker, NEW.context);
END;

CREATE TRIGGER knowledge_cards_ai AFTER INSERT ON knowledge_cards BEGIN
    INSERT INTO knowledge_cards_fts(rowid, concept, explanation) VALUES (NEW.rowid, NEW.concept, NEW.explanation);
END;
```

---

## Skills

### 1. youtube-download

**Syfte:** Ladda ner transkript fr√•n YouTube-videos.

**`.claude/skills/youtube-download/SKILL.md`:**
```markdown
---
name: youtube-download
description: Ladda ner transkript fr√•n YouTube-video. Sparar med beskrivande
  filnamn inkl. kanal, titel och datum.
---

# YouTube Download Skill

## Trigger
Anv√§nd n√§r anv√§ndaren vill ladda ner transkript fr√•n en YouTube-URL.

## Metod
1. Anv√§ndaren ger en YouTube-URL
2. Extrahera video-metadata (titel, kanal, datum, l√§ngd)
3. H√§mta transkript (f√∂redra manuella captions, fallback till auto-generated)
4. Spara till `data/youtube/transcripts/{kanal}-{datum}-{slug}.md`
5. Bekr√§fta f√∂r anv√§ndaren

## Filnamnformat
`huberman-lab-2025-01-15-sleep-optimization.md`

## Dependencies
- yt-dlp eller youtube-transcript-api
```

---

### 2. podcast-download

**Syfte:** Ladda ner podcast-transkript via RSS eller Apple Podcasts.

**`.claude/skills/podcast-download/SKILL.md`:**
```markdown
---
name: podcast-download
description: Ladda ner podcast-transkript. Anv√§nder Apple Podcasts API
  (om tillg√§ngligt) eller Whisper f√∂r transkribering.
---

# Podcast Download Skill

## Trigger
Anv√§nd n√§r anv√§ndaren vill ladda ner transkript fr√•n en podcast-episod.

## Metoder
1. **Apple Podcasts** (f√∂redraget) - Om transkript finns tillg√§ngligt
2. **Whisper** (fallback) - Ladda ner audio och transkribera lokalt

## Fl√∂de
1. Anv√§ndaren ger podcast-namn och episod (eller RSS-URL)
2. S√∂k efter episoden
3. F√∂rs√∂k h√§mta transkript via Apple Podcasts API
4. Om ej tillg√§ngligt: ladda ner audio och k√∂r Whisper
5. Spara till `data/podcasts/transcripts/{podcast}-{datum}-{slug}.md`
```

---

### 3. twitter-download

**Syfte:** H√§mta tweets, tr√•dar och artiklar fr√•n Twitter/X.

**`.claude/skills/twitter-download/SKILL.md`:**
```markdown
---
name: twitter-download
description: Ladda ner tweets, tr√•dar och artiklar fr√•n Twitter/X.
  St√∂djer b√•de enskilda tweets och l√§nkade artiklar.
---

# Twitter Download Skill

## Trigger
Anv√§nd n√§r anv√§ndaren vill samla inneh√•ll fr√•n Twitter.

## Inneh√•llstyper
1. **Enskilda tweets** - Spara tweet-text med metadata
2. **Tr√•dar** - Samla hela tr√•den i ordning
3. **Artiklar** - Extrahera inneh√•ll fr√•n l√§nkade artiklar (Substack, Medium, etc.)

## Metoder
1. **Twitter API** (om tillg√§nglig) - Automatiserat
2. **Browser scraping** (manuellt) - Anv√§ndaren kopierar inneh√•ll

## Filformat
Spara till `data/twitter/sources/{handle}-{datum}-{id}.json`
```

---

### 4. analyze

**Syfte:** Analysera transkript och extrahera insikter.

**`.claude/skills/analyze/SKILL.md`:**
```markdown
---
name: analyze
description: Analysera transkript f√∂r att extrahera takeaways, citat,
  kunskapskort och action items. Tv√• metoder: Claude Code (interaktiv)
  eller OpenCode/GLM-4.7 (batch).
---

# Analyze Skill

## Trigger
Anv√§nd n√§r anv√§ndaren vill analysera ett transkript.

## Steg 1: V√§lj metod
Fr√•ga anv√§ndaren:
1. **Claude Code** - Analysera direkt i konversationen
2. **OpenCode/GLM-4.7** - K√∂r batch-script i separat terminal

## Vid Claude Code
L√§s `references/claude-method.md` och f√∂lj instruktionerna.

## Vid OpenCode
L√§s `references/opencode-method.md` och visa kommandot.

## Output
Sparas som JSON enligt `references/output-schema.md`
```

**`.claude/skills/analyze/references/claude-method.md`:**
```markdown
# Claude Code Analysmetod

## Instruktioner
1. L√§s transkriptet som ska analyseras
2. L√§s kategorierna fr√•n `data/categories.json`
3. Analysera inneh√•llet och extrahera:
   - Summary (2-3 meningar)
   - Takeaways (minst 5, med kategori och taggar)
   - Quotes (minnesv√§rda citat med kontext)
   - Knowledge cards (koncept som f√∂rklaras)
   - Action items (konkreta saker att g√∂ra)
4. Generera UUIDs f√∂r varje item
5. Spara JSON till `data/{source_type}/analyses/{id}.json`
6. Bekr√§fta f√∂r anv√§ndaren

## Prompt-mall
Se output-schema.md f√∂r exakt JSON-format att producera.
```

**`.claude/skills/analyze/references/opencode-method.md`:**
```markdown
# OpenCode/GLM-4.7 Analysmetod

## F√∂ruts√§ttningar
- OpenCode k√∂rs lokalt
- GLM-4.7 modell tillg√§nglig

## Anv√§ndning

### Analysera en specifik fil:
```bash
python scripts/analyze_batch.py --file data/youtube/transcripts/huberman-2025-01-15-sleep.md
```

### Analysera alla v√§ntande transkript:
```bash
python scripts/analyze_batch.py --source youtube --pending
```

### Analysera med specifik modell:
```bash
python scripts/analyze_batch.py --pending --model glm-4.7
```

## Output
Resultat sparas automatiskt till `data/{source}/analyses/`
```

---

### 5. database-sync

**Syfte:** Synka JSON-analyser till SQLite.

**`.claude/skills/database-sync/SKILL.md`:**
```markdown
---
name: database-sync
description: Synka JSON-analysfiler till SQLite-databasen. K√∂r efter
  nya analyser f√∂r att g√∂ra inneh√•llet s√∂kbart.
---

# Database Sync Skill

## Trigger
Anv√§nd efter att nya analyser har skapats.

## Fl√∂de
1. Skanna `data/*/analyses/` efter JSON-filer
2. J√§mf√∂r med befintliga poster i SQLite
3. L√§gg till nya / uppdatera √§ndrade
4. Visa sammanfattning: X nya, Y uppdaterade

## Kommando
```bash
python -m nuggets.db.sync
```
```

---

### 6. search

**Syfte:** S√∂k i kunskapsbiblioteket.

**`.claude/skills/search/SKILL.md`:**
```markdown
---
name: search
description: S√∂k i kunskapsbiblioteket efter takeaways, citat eller koncept.
---

# Search Skill

## Trigger
Anv√§nd n√§r anv√§ndaren vill s√∂ka i sitt kunskapsbibliotek.

## S√∂ktyper
1. **Fulltext** - "s√∂k efter sleep optimization"
2. **Kategori** - "visa alla health takeaways"
3. **K√§lla** - "vad har Huberman sagt?"
4. **Tags** - "hitta allt taggat med 'discipline'"

## Exempel-queries
- "Vad har jag l√§rt mig om morgonrutiner?"
- "Visa citat fr√•n Joe Rogan"
- "Alla kunskapskort i kategorin philosophy"
```

---

## Python-moduler

### `src/nuggets/analyze/prompt_builder.py`

```python
"""Bygger analys-prompts f√∂r b√•de Claude Code och OpenCode."""

import json
from pathlib import Path

def load_categories() -> list[dict]:
    """Ladda kategorier fr√•n categories.json."""
    path = Path("data/categories.json")
    if path.exists():
        return json.loads(path.read_text())["categories"]
    return []

def build_analysis_prompt(
    transcript: str,
    source_metadata: dict,
    categories: list[dict] | None = None
) -> str:
    """
    Bygg prompt f√∂r kunskapsextraktion.

    Args:
        transcript: Transkriptet att analysera
        source_metadata: {"type": "youtube", "title": "...", "author": "..."}
        categories: Lista av kategorier, eller None f√∂r att ladda fr√•n fil

    Returns:
        Komplett prompt f√∂r LLM
    """
    if categories is None:
        categories = load_categories()

    category_list = "\n".join(
        f"- {c['id']}: {c['name']}" for c in categories
    )

    return f'''Analysera f√∂ljande transkript och extrahera v√§rdefulla insikter.

## K√§lla
- Typ: {source_metadata.get("type", "unknown")}
- Titel: {source_metadata.get("title", "Unknown")}
- F√∂rfattare: {source_metadata.get("author", "Unknown")}

## Kategorier
{category_list}

## Uppgift
Extrahera:
1. **Summary** - 2-3 meningar som f√•ngar k√§rnan
2. **Takeaways** - Minst 5 konkreta insikter med kategori och taggar
3. **Quotes** - Minnesv√§rda citat med talare och kontext
4. **Knowledge Cards** - Koncept som f√∂rklaras (concept + explanation)
5. **Action Items** - Konkreta saker man kan g√∂ra

## Output Format
Returnera som JSON enligt detta schema:
{{
  "summary": "...",
  "takeaways": [...],
  "quotes": [...],
  "knowledge_cards": [...],
  "action_items": [...]
}}

## Transkript
{transcript}
'''
```

### `scripts/analyze_batch.py`

```python
#!/usr/bin/env python3
"""
Batch-analys av transkript via OpenCode/GLM-4.7.

Anv√§ndning:
    python scripts/analyze_batch.py --source youtube --pending
    python scripts/analyze_batch.py --file path/to/transcript.md
"""

import argparse
import json
import os
from datetime import datetime
from pathlib import Path
import uuid

# Konfigurera via environment
OPENCODE_API_URL = os.getenv("OPENCODE_API_URL", "http://localhost:11434")
OPENCODE_MODEL = os.getenv("OPENCODE_MODEL", "glm-4.7")

def find_pending_transcripts(source: str) -> list[Path]:
    """Hitta transkript som saknar analys."""
    transcript_dir = Path(f"data/{source}/transcripts")
    analysis_dir = Path(f"data/{source}/analyses")

    if not transcript_dir.exists():
        return []

    pending = []
    for transcript in transcript_dir.glob("*.md"):
        analysis_path = analysis_dir / f"{transcript.stem}.json"
        if not analysis_path.exists():
            pending.append(transcript)

    return pending

def analyze_transcript(transcript_path: Path, model: str = OPENCODE_MODEL) -> dict:
    """Analysera ett transkript via OpenCode."""
    from nuggets.analyze.prompt_builder import build_analysis_prompt

    # L√§s transkript
    transcript = transcript_path.read_text()

    # Extrahera metadata fr√•n filnamn
    # Format: author-YYYY-MM-DD-slug.md
    parts = transcript_path.stem.split("-")
    source_metadata = {
        "type": transcript_path.parent.parent.name,
        "title": " ".join(parts[4:]).replace("-", " ").title() if len(parts) > 4 else transcript_path.stem,
        "author": parts[0] if parts else "Unknown",
        "date": f"{parts[1]}-{parts[2]}-{parts[3]}" if len(parts) >= 4 else None
    }

    # Bygg prompt
    prompt = build_analysis_prompt(transcript, source_metadata)

    # Anropa OpenCode/GLM-4.7
    # TODO: Implementera faktiskt API-anrop
    # response = call_opencode(prompt, model)

    # Placeholder f√∂r nu
    print(f"Would analyze: {transcript_path}")
    print(f"Using model: {model}")

    return {}

def main():
    parser = argparse.ArgumentParser(description="Batch-analysera transkript")
    parser.add_argument("--source", choices=["youtube", "podcasts", "twitter"])
    parser.add_argument("--file", type=Path, help="Specifik fil att analysera")
    parser.add_argument("--pending", action="store_true", help="Analysera alla v√§ntande")
    parser.add_argument("--model", default=OPENCODE_MODEL)

    args = parser.parse_args()

    if args.file:
        analyze_transcript(args.file, args.model)
    elif args.pending and args.source:
        pending = find_pending_transcripts(args.source)
        print(f"Hittade {len(pending)} v√§ntande transkript")
        for path in pending:
            analyze_transcript(path, args.model)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
```

---

## Implementation - N√§sta steg

> **OBS:** Innan implementation, unders√∂k vad som redan finns i repot och anpassa planen d√§refter.

### Fas 1: Grundstruktur
1. Skapa mappstruktur enligt ovan
2. Skapa `data/categories.json` med initiala kategorier
3. S√§tt upp `pyproject.toml` med dependencies

### Fas 2: Download Skills
1. Implementera `youtube-download` skill
2. Implementera `podcast-download` skill
3. Implementera `twitter-download` skill

### Fas 3: Analys
1. Skapa `src/nuggets/analyze/` moduler
2. Implementera `analyze` skill med b√•da metoder
3. Skapa `scripts/analyze_batch.py`

### Fas 4: Databas
1. Skapa SQLite-schema
2. Implementera `src/nuggets/db/sync.py`
3. Skapa `database-sync` skill

### Fas 5: S√∂kning
1. Implementera `search` skill
2. L√§gg till FTS-queries

### Fas 6: Export (framtid)
1. Apple Notes export
2. Vector embeddings f√∂r semantisk s√∂kning

---

## Tekniska dependencies

```toml
[project]
dependencies = [
    "yt-dlp",              # YouTube downloads
    "youtube-transcript-api",
    "pydantic>=2.0",       # Validering
    "sqlalchemy>=2.0",     # Database ORM
    "rich",                # CLI output
]

[project.optional-dependencies]
whisper = [
    "mlx-whisper",         # Apple Silicon transkribering
]
```

---

## Inspiration fr√•n PodStock

F√∂ljande filer fr√•n `finance-agent` kan anv√§ndas som referens:
- `.claude/skills/analyze/` - Skill-struktur och dual-method approach
- `.claude/skills/youtube-download/` - YouTube-nedladdning
- `.claude/skills/twitter-download/` - Twitter-hantering
- `src/podstock/db/` - SQLAlchemy-modeller och sync-logik
- `scripts/` - Batch-scripts f√∂r OpenCode
